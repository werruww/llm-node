{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ad9e9cda12a401a9608f6c177f18294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e950231ffdcd4e0294a9aed63917eb78",
              "IPY_MODEL_a50cf094429e45248d942618932b08a5",
              "IPY_MODEL_e7a335a7a6eb449897ed9f6e3f4bd7cc"
            ],
            "layout": "IPY_MODEL_363e06975d3e4f899d19552cdbae6d81"
          }
        },
        "e950231ffdcd4e0294a9aed63917eb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_507f3fd32f2549cd9a1ba7ce93923229",
            "placeholder": "​",
            "style": "IPY_MODEL_66c78edf767c475880a0bfde3cbd4e64",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a50cf094429e45248d942618932b08a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36bcedcbec424d6d8fbaa6e8fd4ca7cd",
            "max": 6634,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68a2401cde57439d9ee5cbb0ef997184",
            "value": 6634
          }
        },
        "e7a335a7a6eb449897ed9f6e3f4bd7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614344dcc0d7433f9e98587ba1fcc558",
            "placeholder": "​",
            "style": "IPY_MODEL_6801831db0eb4f828c083c7cbf6c966e",
            "value": " 6.63k/6.63k [00:00&lt;00:00, 441kB/s]"
          }
        },
        "363e06975d3e4f899d19552cdbae6d81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "507f3fd32f2549cd9a1ba7ce93923229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c78edf767c475880a0bfde3cbd4e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36bcedcbec424d6d8fbaa6e8fd4ca7cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a2401cde57439d9ee5cbb0ef997184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "614344dcc0d7433f9e98587ba1fcc558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6801831db0eb4f828c083c7cbf6c966e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94d99225d4e74b358717dc60b7d95eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8920e8f0b5d427cafba59ab3253e662",
              "IPY_MODEL_04fd3b9500364967a02f29628cda18c4",
              "IPY_MODEL_4a3c52153c194eed883fcc772321378e"
            ],
            "layout": "IPY_MODEL_ff1de6d475914d76afd6cd0a174cad03"
          }
        },
        "c8920e8f0b5d427cafba59ab3253e662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25d88f228f7f4305b8535b5866b98325",
            "placeholder": "​",
            "style": "IPY_MODEL_1db0b0449db44a18bab6dbe14ea32775",
            "value": "config.json: 100%"
          }
        },
        "04fd3b9500364967a02f29628cda18c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_968fe5dad89c499e8ed16b26fdc012ad",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_837327f8ef2b4da6a2f63c2fa1c5581f",
            "value": 629
          }
        },
        "4a3c52153c194eed883fcc772321378e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1696f0004eff434c84412cc0894487e0",
            "placeholder": "​",
            "style": "IPY_MODEL_a29840c26b434bfbb115838bed902ccb",
            "value": " 629/629 [00:00&lt;00:00, 41.1kB/s]"
          }
        },
        "ff1de6d475914d76afd6cd0a174cad03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d88f228f7f4305b8535b5866b98325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1db0b0449db44a18bab6dbe14ea32775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "968fe5dad89c499e8ed16b26fdc012ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "837327f8ef2b4da6a2f63c2fa1c5581f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1696f0004eff434c84412cc0894487e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a29840c26b434bfbb115838bed902ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84fcebcdbe4241fdaaa9bb6b6b1d1482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a015ae13575d40b696e3f4768d6e8917",
              "IPY_MODEL_3be6c08f610048a3af7283ce5b8febd2",
              "IPY_MODEL_954ad9192c704b4ea433d75e2c0946f8"
            ],
            "layout": "IPY_MODEL_e2f48c331b654469998c1daefcdfb18d"
          }
        },
        "a015ae13575d40b696e3f4768d6e8917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d39e124cde4040d18ad3055af9983a83",
            "placeholder": "​",
            "style": "IPY_MODEL_4219c602ee1a42c2a6fc3b38f22be491",
            "value": "model.safetensors: 100%"
          }
        },
        "3be6c08f610048a3af7283ce5b8febd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69900db3b72a4191b93c013c5fc9658f",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f7888fa538b4fcb96414df032816625",
            "value": 267832558
          }
        },
        "954ad9192c704b4ea433d75e2c0946f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c0fa39d1e754867b1cb456896f22dd9",
            "placeholder": "​",
            "style": "IPY_MODEL_911a80d97a7041978f20933d42b6654e",
            "value": " 268M/268M [00:02&lt;00:00, 112MB/s]"
          }
        },
        "e2f48c331b654469998c1daefcdfb18d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d39e124cde4040d18ad3055af9983a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4219c602ee1a42c2a6fc3b38f22be491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69900db3b72a4191b93c013c5fc9658f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f7888fa538b4fcb96414df032816625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c0fa39d1e754867b1cb456896f22dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "911a80d97a7041978f20933d42b6654e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1e913b93d9f4ef1905aa5d2f1216574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07456077e17b494c8f2524ab5056dd6c",
              "IPY_MODEL_a6931a8fe57e4e50ae0dcfd41907530c",
              "IPY_MODEL_463caf3c561b4c7cbce3ebb6d2668d99"
            ],
            "layout": "IPY_MODEL_be32c8153c0245c39ea6b66da1ae11cb"
          }
        },
        "07456077e17b494c8f2524ab5056dd6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d8c398e20e04fdb8ff91ada95601010",
            "placeholder": "​",
            "style": "IPY_MODEL_7e0a8562c6a4455c8cf991fde4641a43",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a6931a8fe57e4e50ae0dcfd41907530c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87376f17e4e74d38a9c59eb5845b2919",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6e957d215c1419f83ef20b4e8bbff1c",
            "value": 48
          }
        },
        "463caf3c561b4c7cbce3ebb6d2668d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce7fdde86a8a4700a7506edfe0f44747",
            "placeholder": "​",
            "style": "IPY_MODEL_668f3f9d5c934a7ab5aaabeac8f942b4",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.41kB/s]"
          }
        },
        "be32c8153c0245c39ea6b66da1ae11cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d8c398e20e04fdb8ff91ada95601010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0a8562c6a4455c8cf991fde4641a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87376f17e4e74d38a9c59eb5845b2919": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e957d215c1419f83ef20b4e8bbff1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce7fdde86a8a4700a7506edfe0f44747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "668f3f9d5c934a7ab5aaabeac8f942b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d05401d92ffa4292b6ff6f001e493d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_991466e438a7497c9691e862a55857ad",
              "IPY_MODEL_492a47f12a394d008c7d30c61f185eab",
              "IPY_MODEL_3d5c9a1970ab4fee8cfca2ccce86bd4f"
            ],
            "layout": "IPY_MODEL_ec2aac32a4764525bc66cc38ec4f6a88"
          }
        },
        "991466e438a7497c9691e862a55857ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46cedc2a32864ec78f62403f5a48818a",
            "placeholder": "​",
            "style": "IPY_MODEL_826d7d33d2de437082b90086f2651f02",
            "value": "vocab.txt: 100%"
          }
        },
        "492a47f12a394d008c7d30c61f185eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faf23ace225745a59b40e3b099c99c4c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5142fe227bb4a20ace3cd6ecec77201",
            "value": 231508
          }
        },
        "3d5c9a1970ab4fee8cfca2ccce86bd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_564991ab89414ca992d568ec49261878",
            "placeholder": "​",
            "style": "IPY_MODEL_742dd84049e44286a5f44807769ba8b4",
            "value": " 232k/232k [00:00&lt;00:00, 4.99MB/s]"
          }
        },
        "ec2aac32a4764525bc66cc38ec4f6a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46cedc2a32864ec78f62403f5a48818a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "826d7d33d2de437082b90086f2651f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faf23ace225745a59b40e3b099c99c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5142fe227bb4a20ace3cd6ecec77201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "564991ab89414ca992d568ec49261878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "742dd84049e44286a5f44807769ba8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDJJOM1TedMz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NMg0RykEef4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FscGTFPzef7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GeWVLOnmemzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6JsJjhI1em2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boZ4GlKjem5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm i @huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzqDAUzXaNU9",
        "outputId": "1ffd3680-81ee-4ccf-bb88-f7719cc84b3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "added 80 packages in 42s\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K18 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "microsoft/Phi-3.5-mini-instruct"
      ],
      "metadata": {
        "id": "lXxC5kide63X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install optimum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc-3MfnCbK11",
        "outputId": "2b7829bf-c3ff-44d6-d5ff-e162fe464b5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum\n",
            "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum) (4.48.2)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum) (2.5.1+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2025.1.31)\n",
            "Downloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, optimum\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 optimum-1.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bHMNqryEaN6O",
        "outputId": "a432a354-fab6-4ff2-f4ae-506f331c37dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optimum[onnxruntime] in /usr/local/lib/python3.11/dist-packages (1.24.0)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.48.2)\n",
            "Collecting transformers>=4.29 (from optimum[onnxruntime])\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.5.1+cu124)\n",
            "Collecting torch>=1.11 (from optimum[onnxruntime])\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (1.26.4)\n",
            "Collecting numpy (from optimum[onnxruntime])\n",
            "  Downloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (0.28.1)\n",
            "Collecting onnx (from optimum[onnxruntime])\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime])\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting datasets>=1.2.1 (from optimum[onnxruntime])\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting evaluate (from optimum[onnxruntime])\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.25.6)\n",
            "Collecting protobuf>=3.20.1 (from optimum[onnxruntime])\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (17.0.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading pyarrow-19.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.2.2)\n",
            "Collecting pandas (from datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.67.1)\n",
            "Collecting xxhash (from datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.11.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.12.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.11.0->optimum[onnxruntime])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (25.1.24)\n",
            "Collecting flatbuffers (from onnxruntime>=1.11.0->optimum[onnxruntime])\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (1.13.1)\n",
            "Collecting sympy (from onnxruntime>=1.11.0->optimum[onnxruntime])\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.3.1.170)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=1.11->optimum[onnxruntime])\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch>=1.11->optimum[onnxruntime])\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.11.0->optimum[onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (2.4.4)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2025.1.31)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.11.0->optimum[onnxruntime])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.8.2)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->datasets>=1.2.1->optimum[onnxruntime])\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.2.1->optimum[onnxruntime]) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-19.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, flatbuffers, xxhash, python-dateutil, pyarrow, protobuf, numpy, humanfriendly, fsspec, dill, aiohappyeyeballs, pandas, onnx, multiprocess, coloredlogs, torch, onnxruntime, transformers, datasets, evaluate\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.1.24\n",
            "    Uninstalling flatbuffers-25.1.24:\n",
            "      Successfully uninstalled flatbuffers-25.1.24\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: aiohappyeyeballs\n",
            "    Found existing installation: aiohappyeyeballs 2.4.4\n",
            "    Uninstalling aiohappyeyeballs-2.4.4:\n",
            "      Successfully uninstalled aiohappyeyeballs-2.4.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.2\n",
            "    Uninstalling transformers-4.48.2:\n",
            "      Successfully uninstalled transformers-4.48.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "langchain 0.3.17 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.2 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.2 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.2 which is incompatible.\n",
            "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.2 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.2 which is incompatible.\n",
            "pylibcudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.6 coloredlogs-15.0.1 datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 flatbuffers-25.2.10 fsspec-2024.9.0 humanfriendly-10.0 multiprocess-0.70.16 numpy-2.2.2 nvidia-cusparselt-cu12-0.6.2 onnx-1.17.0 onnxruntime-1.20.1 pandas-2.2.3 protobuf-5.29.3 pyarrow-19.0.0 python-dateutil-2.9.0.post0 torch-2.6.0 transformers-4.48.3 triton-3.2.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              },
              "id": "a7a08fc0883940ed93163be776396887"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "!pip install --upgrade torchvision"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjKb1c8ochTh",
        "outputId": "f857de10-24ba-4df7-e67e-22ab3b27d330"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchvision-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX"
      ],
      "metadata": {
        "id": "YSTZZDm5fLGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=\"model_quantized.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "results = classifier(\"I love burritos!\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Kx6Lox4Xef9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline, TextStreamer } from \"@huggingface/transformers\";\n",
        "\n",
        "// Create a text generation pipeline\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\n",
        "  { dtype: \"q4f16\" },\n",
        ");\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  { role: \"user\", content:  \"Solve the equation: x^2 - 3x + 2 = 0\" },\n",
        "];\n",
        "\n",
        "// Create text streamer\n",
        "const streamer = new TextStreamer(generator.tokenizer, {\n",
        "  skip_prompt: true,\n",
        "  // callback_function: (text) => { }, // Optional callback function\n",
        "})\n",
        "\n",
        "// Generate a response\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false, streamer });\n",
        "console.log(output[0].generated_text.at(-1).content);\n"
      ],
      "metadata": {
        "id": "RwGJGGtZfWaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/1.js"
      ],
      "metadata": {
        "id": "opfDM6XKgsuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm i @huggingface/transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDsK_yucgvGL",
        "outputId": "9e42276a-4ca7-4de1-eb59-bd9f7344950b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
            "added 2 packages, and audited 83 packages in 3s\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K20 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ayhgشغال"
      ],
      "metadata": {
        "id": "hvlcUe7EhfHv"
      }
    },
    {
      "source": [
        "#/content/package.json\n",
        " {\n",
        "  \"dependencies\": {\n",
        "    \"@huggingface/transformers\": \"^3.3.3\"\n",
        "  },\n",
        "  \"type\": \"module\"\n",
        "}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fgZqPc-XhU7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/1.js"
      ],
      "metadata": {
        "id": "nQNwSQIJk97C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually Download the Model"
      ],
      "metadata": {
        "id": "TzL9Y79plUE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "snapshot_download(repo_id=\"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\")\n"
      ],
      "metadata": {
        "id": "iKpgeE03k-UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"/path/to/local/model\",\n",
        "  { dtype: \"fp16\" }\n",
        ");\n"
      ],
      "metadata": {
        "id": "TuEKbRAxlfAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nCHHD9imLma",
        "outputId": "b9978f62-9a4e-4a49-aa0c-d0cdf8fe6901"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install onnxruntime-node\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysQ27p-EmvaK",
        "outputId": "e7caec7f-61e3-4302-b89e-8ca60a2e4633"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "up to date, audited 83 packages in 2s\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K20 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install onnxruntime-web\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9UkHooZmyCC",
        "outputId": "985c3760-099a-4c5e-b5a9-f64126a6da24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n",
            "up to date, audited 83 packages in 3s\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K20 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/1.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwiVummTmMMK",
        "outputId": "d0ac8f23-7556-4f77-c4e4-9f931b9fb49b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@huggingface/transformers/dist/transformers.mjs:31095\n",
            "    throw Error(`${message}: \"${remoteURL}\".`);\n",
            "          ^\n",
            "\n",
            "Error: Unauthorized access to file: \"https://huggingface.co/onnx-community/DeepSeek-R1-Qwen-1.5B-ONNX/resolve/main/tokenizer_config.json\".\n",
            "    at handleError \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31095:11\u001b[90m)\u001b[39m\n",
            "    at getModelFile \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31328:24\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async getModelJSON \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31427:18\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 1)\n",
            "    at async loadTokenizer \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24731:18\u001b[90m)\u001b[39m\n",
            "    at async AutoTokenizer.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:29020:50\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 0)\n",
            "    at async loadItems \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24602:5\u001b[90m)\u001b[39m\n",
            "    at async pipeline \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24532:21\u001b[90m)\u001b[39m\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "// npm i @huggingface/transformers\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate pipeline\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA');"
      ],
      "metadata": {
        "id": "YTuNQ6E8nHh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تحميل ملف معين"
      ],
      "metadata": {
        "id": "tiIA4ZTtnnJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# تحميل ملف معين فقط من النموذج\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\n",
        "    filename=\"tokenizer_config.json\",  # اسم الملف المطلوب\n",
        "    cache_dir=\"./model_files\"  # تحديد مجلد التخزين المحلي (اختياري)\n",
        ")\n",
        "\n",
        "print(f\"تم تحميل الملف في: {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "4ad9e9cda12a401a9608f6c177f18294",
            "e950231ffdcd4e0294a9aed63917eb78",
            "a50cf094429e45248d942618932b08a5",
            "e7a335a7a6eb449897ed9f6e3f4bd7cc",
            "363e06975d3e4f899d19552cdbae6d81",
            "507f3fd32f2549cd9a1ba7ce93923229",
            "66c78edf767c475880a0bfde3cbd4e64",
            "36bcedcbec424d6d8fbaa6e8fd4ca7cd",
            "68a2401cde57439d9ee5cbb0ef997184",
            "614344dcc0d7433f9e98587ba1fcc558",
            "6801831db0eb4f828c083c7cbf6c966e"
          ]
        },
        "id": "KUOtW8kunkxj",
        "outputId": "cb9bd13a-1cc1-4127-c3e9-94ef8a38ec57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/6.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ad9e9cda12a401a9608f6c177f18294"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تم تحميل الملف في: ./model_files/models--onnx-community--DeepSeek-R1-Distill-Qwen-1.5B-ONNX/snapshots/f9c94fd59ec97bdb5e7587d09343797481a8c385/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.onnx_data"
      ],
      "metadata": {
        "id": "0hA3U9t-n60C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# تحميل ملف معين فقط من النموذج\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\n",
        "    filename=\"model.onnx\",  # اسم الملف المطلوب\n",
        "    cache_dir=\"./model_files\"  # تحديد مجلد التخزين المحلي (اختياري)\n",
        ")\n",
        "\n",
        "print(f\"تم تحميل الملف في: {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "qWnx9Qt8n8Ol",
        "outputId": "9e2dae9a-18f4-482e-907f-088f04b55c3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EntryNotFoundError",
          "evalue": "404 Client Error. (Request ID: Root=1-67abae26-27e47f24620843c71c039a76;4ee62d4b-b507-454e-aa4d-8ba85faeb19b)\n\nEntry Not Found for url: https://huggingface.co/onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX/resolve/main/model.onnx.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX/resolve/main/model.onnx",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0a3e0a7878ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# تحميل ملف معين فقط من النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m file_path = hf_hub_download(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# اسم الملف المطلوب\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;31m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0;31m# If we can't, a HEAD request error is returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m     (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# Recursively follow relative redirects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0merror_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"EntryNotFound\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{response.status_code} Client Error.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"Entry Not Found for url: {response.url}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEntryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0merror_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GatedRepo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67abae26-27e47f24620843c71c039a76;4ee62d4b-b507-454e-aa4d-8ba85faeb19b)\n\nEntry Not Found for url: https://huggingface.co/onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX/resolve/main/model.onnx."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://onnxruntime.ai/"
      ],
      "metadata": {
        "id": "gFqcxLtLoRpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIbbdni_n-sj",
        "outputId": "7cb97a48-9516-45d0-87d7-c9f06b9997b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_h2suZ-oODL",
        "outputId": "e5689941-6ded-42aa-c9f6-d4014b3f964f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-genai\n",
            "  Downloading onnxruntime_genai-0.5.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (648 bytes)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-genai) (2.2.2)\n",
            "Requirement already satisfied: onnxruntime>=1.20.1 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-genai) (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.20.1->onnxruntime-genai) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.20.1->onnxruntime-genai) (1.3.0)\n",
            "Downloading onnxruntime_genai-0.5.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-genai\n",
            "Successfully installed onnxruntime-genai-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/microsoft/onnxruntime/tree/main/js/node"
      ],
      "metadata": {
        "id": "gEDJI--3pKXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install onnxruntime-node"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nApcn9lLoP5L",
        "outputId": "c274a6ad-1f91-43c6-ace2-be9b7657e68d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "up to date, audited 83 packages in 3s\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K20 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/microsoft/onnxruntime-inference-examples/tree/main/js"
      ],
      "metadata": {
        "id": "1wVEp5MNpOk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/onnxruntime-inference-examples.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9sgnit1pCDM",
        "outputId": "025d22e1-bf58-4ed4-aa1b-3e5cd20c83ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'onnxruntime-inference-examples'...\n",
            "remote: Enumerating objects: 5780, done.\u001b[K\n",
            "remote: Counting objects: 100% (1059/1059), done.\u001b[K\n",
            "remote: Compressing objects: 100% (260/260), done.\u001b[K\n",
            "remote: Total 5780 (delta 899), reused 804 (delta 797), pack-reused 4721 (from 1)\u001b[K\n",
            "Receiving objects: 100% (5780/5780), 438.46 MiB | 20.88 MiB/s, done.\n",
            "Resolving deltas: 100% (2421/2421), done.\n",
            "Updating files: 100% (1014/1014), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-inference-examples/js/chat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFyBYOdqpdR7",
        "outputId": "7304d8d7-36cd-48d6-a06c-7bf4590b960d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-inference-examples/js/chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeB1UZ_GpqnR",
        "outputId": "647c7494-21f2-48a0-c646-37e80d812716"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "added 399 packages, and audited 400 packages in 1m\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K72 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm run build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MJq-kfWprM7",
        "outputId": "c4559037-2b02-4db2-9abe-674e987dbbf3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> localchat@0.0.0 build\n",
            "> webpack\n",
            "\n",
            "\u001b[1G\u001b[0Kassets by path \u001b[1m\u001b[32mdist/*.wasm\u001b[39m\u001b[22m 37.3 MiB\n",
            "  asset \u001b[1m\u001b[32mdist/ort-wasm-simd-threaded.jsep.wasm\u001b[39m\u001b[22m 19.7 MiB \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m [from: node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm] \u001b[1m\u001b[32m[copied]\u001b[39m\u001b[22m\n",
            "  asset \u001b[1m\u001b[32mdist/ort-wasm-simd.jsep.wasm\u001b[39m\u001b[22m 17.6 MiB \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m [from: node_modules/onnxruntime-web/dist/ort-wasm-simd.jsep.wasm] \u001b[1m\u001b[32m[copied]\u001b[39m\u001b[22m\n",
            "assets by path \u001b[1m\u001b[32mdist/*.js\u001b[39m\u001b[22m 4.75 MiB\n",
            "  asset \u001b[1m\u001b[32mdist/main.min.js\u001b[39m\u001b[22m 2.37 MiB \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m [javascript module] (name: dist/main.min) 1 related asset\n",
            "  asset \u001b[1m\u001b[32mdist/main.js\u001b[39m\u001b[22m 2.37 MiB \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m [javascript module] (name: dist/main) 1 related asset\n",
            "runtime modules 3.54 KiB 8 modules\n",
            "modules by path \u001b[1m./\u001b[39m\u001b[22m 2.22 MiB\n",
            "  modules by path \u001b[1m./node_modules/\u001b[39m\u001b[22m 2.21 MiB 30 modules\n",
            "  modules by path \u001b[1m./*.js\u001b[39m\u001b[22m 16.5 KiB\n",
            "    \u001b[1m./main.js\u001b[39m\u001b[22m 9.02 KiB \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "    \u001b[1m./llm.js\u001b[39m\u001b[22m 7.5 KiB \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "\u001b[1mfs (ignored)\u001b[39m\u001b[22m 15 bytes \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "\u001b[1mpath (ignored)\u001b[39m\u001b[22m 15 bytes \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "\u001b[1murl (ignored)\u001b[39m\u001b[22m 15 bytes \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "\u001b[1msharp (ignored)\u001b[39m\u001b[22m 15 bytes \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "\u001b[1mfs (ignored)\u001b[39m\u001b[22m 15 bytes \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "\u001b[1mpath (ignored)\u001b[39m\u001b[22m 15 bytes \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "\u001b[1monnxruntime-node (ignored)\u001b[39m\u001b[22m 15 bytes \u001b[1m\u001b[33m[built]\u001b[39m\u001b[22m \u001b[1m\u001b[33m[code generated]\u001b[39m\u001b[22m\n",
            "webpack 5.97.1 compiled \u001b[1m\u001b[32msuccessfully\u001b[39m\u001b[22m in 3220 ms\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm run dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRzqSC6kptu4",
        "outputId": "637b10a9-ad99-4ae9-fdff-72dc0b823c51"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> localchat@0.0.0 dev\n",
            "> webpack serve --no-client-overlay\n",
            "\n",
            "\u001b[1G\u001b[0K/content/onnxruntime-inference-examples/js/chat/node_modules/webpack-dev-server/lib/Server.js:2602\n",
            "        throw error;\n",
            "        ^\n",
            "\n",
            "Error: listen EADDRINUSE: address already in use :::8080\n",
            "\u001b[90m    at Server.setupListenHandle [as _listen2] (node:net:1817:16)\u001b[39m\n",
            "\u001b[90m    at listenInCluster (node:net:1865:12)\u001b[39m\n",
            "\u001b[90m    at Server.listen (node:net:1953:7)\u001b[39m\n",
            "    at READ_WRITE \u001b[90m(/content/onnxruntime-inference-examples/js/chat/\u001b[39mnode_modules/\u001b[4mwebpack-dev-server\u001b[24m/lib/Server.js:3349:23\u001b[90m)\u001b[39m\n",
            "    at new Promise (<anonymous>)\n",
            "    at Server.start \u001b[90m(/content/onnxruntime-inference-examples/js/chat/\u001b[39mnode_modules/\u001b[4mwebpack-dev-server\u001b[24m/lib/Server.js:3347:7\u001b[90m)\u001b[39m\n",
            "    at async Command.<anonymous> \u001b[90m(/content/onnxruntime-inference-examples/js/chat/\u001b[39mnode_modules/\u001b[4m@webpack-cli\u001b[24m/serve/lib/index.js:158:21\u001b[90m)\u001b[39m\n",
            "    at async Command.parseAsync \u001b[90m(/content/onnxruntime-inference-examples/js/chat/\u001b[39mnode_modules/\u001b[4mwebpack-cli\u001b[24m/node_modules/\u001b[4mcommander\u001b[24m/lib/command.js:935:5\u001b[90m)\u001b[39m\n",
            "    at async Command.<anonymous> \u001b[90m(/content/onnxruntime-inference-examples/js/chat/\u001b[39mnode_modules/\u001b[4mwebpack-cli\u001b[24m/lib/webpack-cli.js:1356:13\u001b[90m)\u001b[39m\n",
            "    at async Command.parseAsync \u001b[90m(/content/onnxruntime-inference-examples/js/chat/\u001b[39mnode_modules/\u001b[4mwebpack-cli\u001b[24m/node_modules/\u001b[4mcommander\u001b[24m/lib/command.js:935:5\u001b[90m)\u001b[39m {\n",
            "  code: \u001b[32m'EADDRINUSE'\u001b[39m,\n",
            "  errno: \u001b[33m-98\u001b[39m,\n",
            "  syscall: \u001b[32m'listen'\u001b[39m,\n",
            "  address: \u001b[32m'::'\u001b[39m,\n",
            "  port: \u001b[33m8080\u001b[39m\n",
            "}\n",
            "\n",
            "Node.js v18.20.5\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://onnxruntime.ai/docs/build/inferencing.html"
      ],
      "metadata": {
        "id": "T62oTyOeqMZc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2v8ATkfqM6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/optimum/quicktour"
      ],
      "metadata": {
        "id": "sMDsHVwDqyMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export onnx --model gpt2 gpt2_onnx/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbXFlHFaqzMc",
        "outputId": "3c64979f-2bc8-4a87-ee5f-c4d7d84916b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "2025-02-11 20:20:44.382951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739305244.684025   14238 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739305244.764837   14238 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-11 20:20:45.366998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.08MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:08<00:00, 65.3MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 612kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 166kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.98MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 2.59MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 7.02MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
            "/usr/local/lib/python3.11/dist-packages/optimum/exporters/onnx/model_patcher.py:454: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if past_key_values_length > 0:\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_3510'}\n",
            "\ttransformer.wte.weight: {'transformer.wte.weight'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/onnxruntime-inference-examples/js/chat/gpt2_onnx /content/gpt2_onnx"
      ],
      "metadata": {
        "id": "10WZoxZ7rk4k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=\"/content/gpt2_onnx/model.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "results = classifier(\"I love burritos!\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "oojl9ITrrByb",
        "outputId": "5becf2ce-448a-4c24-ef9b-86fe675471c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'save_directory' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b47f10e16ba4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORTModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/gpt2_onnx/model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_directory' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=\"/content/gpt2_onnx/model.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text\", model=model, tokenizer=tokenizer)\n",
        "results = classifier(\"who is ai?\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "OWTEqAjzsAL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from optimum.onnxruntime import ORTModelForCausalLM  # Or ORTModel if necessary\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "model = ORTModelForCausalLM.from_pretrained(save_directory, file_name=\"/content/gpt2_onnx/model.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "results = classifier(\"who is ai?\")\n",
        "print(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "U79Nes5nsUXW",
        "outputId": "43d72ea5-edbd-46c6-c4ca-8f96b0fb3377"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'save_directory' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e4dbc5021546>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORTModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/gpt2_onnx/model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_directory' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForCausalLM  # Or ORTModel if necessary\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "model = ORTModelForCausalLM.from_pretrained(save_directory, file_name=\"/content/gpt2_onnx/model.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "results = classifier(\"who is ai?\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "zVos_WeKsaJg",
        "outputId": "72b44ba2-378b-4bd6-961c-18e68d92b50b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'save_directory' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e4dbc5021546>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORTModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/gpt2_onnx/model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_directory' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال من مسار"
      ],
      "metadata": {
        "id": "75kRwh0vssKF"
      }
    },
    {
      "source": [
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "save_directory = \"/content/gpt2_onnx\"  # Replace with your actual path\n",
        "\n",
        "model = ORTModelForCausalLM.from_pretrained(save_directory, file_name=\"model.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "results = classifier(\"who is ai?\")\n",
        "print(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heYISAD5sk8R",
        "outputId": "061a8200-013d-47bb-9cf7-53b2ddd2a358"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'who is ai?\\n\\nWhat is this?\\n\\nIs this a \"special treatment?\" And what is your response once it\\'s done?\\n\\nWhere it goes\\n\\nWhen you\\'ve done this interview in person, the first day,'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline, TextStreamer } from \"@huggingface/transformers\";\n",
        "\n",
        "// Create a text generation pipeline\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\n",
        "  { dtype: \"q4f16\" },\n",
        ");\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  { role: \"user\", content:  \"Solve the equation: x^2 - 3x + 2 = 0\" },\n",
        "];\n",
        "\n",
        "// Create text streamer\n",
        "const streamer = new TextStreamer(generator.tokenizer, {\n",
        "  skip_prompt: true,\n",
        "  // callback_function: (text) => { }, // Optional callback function\n",
        "})\n",
        "\n",
        "// Generate a response\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false, streamer });\n",
        "console.log(output[0].generated_text.at(-1).content);\n"
      ],
      "metadata": {
        "id": "64nx4cGNsqOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_READ_TOKEN'] = 'x'"
      ],
      "metadata": {
        "id": "yQFr05-_taDW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/1.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uVKJZAZtLsV",
        "outputId": "73bb27ec-4628-48db-9957-5ec4a0350e50"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@huggingface/transformers/dist/transformers.mjs:31095\n",
            "    throw Error(`${message}: \"${remoteURL}\".`);\n",
            "          ^\n",
            "\n",
            "Error: Unauthorized access to file: \"https://huggingface.co/content/gpt2_onnx/resolve/main/tokenizer_config.json\".\n",
            "    at handleError (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31095:11)\n",
            "    at getModelFile (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31328:24)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async getModelJSON (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31427:18)\n",
            "    at async Promise.all (index 1)\n",
            "    at async loadTokenizer (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24731:18)\n",
            "    at async AutoTokenizer.from_pretrained (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:29020:50)\n",
            "    at async Promise.all (index 0)\n",
            "    at async loadItems (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24602:5)\n",
            "    at async pipeline (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24532:21)\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"/content/gpt2_onnx\", // Path to your locally downloaded model\n",
        ");\n",
        "\n",
        "const prompt = \"Write a story about a cat who goes on an adventure:\";\n",
        "\n",
        "const generatedText = await generator(prompt, {\n",
        "  max_new_tokens: 100, // Adjust as needed\n",
        "});\n",
        "\n",
        "console.log(generatedText[0].generated_text); // Print the generated text"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wlpouYCgx3BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"./deepseek_model\", // Path to your locally downloaded model\n",
        ");"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "k_eRsgOoxNlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "- from transformers import AutoModelForSequenceClassification\n",
        "+ from optimum.intel.openvino import OVModelForSequenceClassification\n",
        "  from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "  # Download a tokenizer and model from the Hub and convert to OpenVINO format\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "- model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
        "+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\n",
        "\n",
        "  # Run inference!\n",
        "  classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "  results = classifier(\"He's a dreadful magician.\")"
      ],
      "metadata": {
        "id": "T0V0dCjmtOYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "- from transformers import AutoModelForSequenceClassification\n",
        "+ from optimum.intel.openvino import OVModelForSequenceClassification\n",
        "  from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "  # Download a tokenizer and model from the Hub and convert to OpenVINO format\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "- model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
        "+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\n",
        "\n",
        "  # Run inference!\n",
        "  classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "  results = classifier(\"He's a dreadful magician.\")"
      ],
      "metadata": {
        "id": "p5-3NBYkyKfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "results = classifier(\"He's a dreadful magician.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "b7BbKeuOyPQf",
        "outputId": "66b547af-56ca-46c1-c739-a1b3878b93c3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_id' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-230fcbeb2e13>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"distilbert-base-uncased-finetuned-sst-2-english\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOVModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_id' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "# --------------------------------------------------------------------------\n",
        "from __future__ import annotations\n",
        "\n",
        "import collections\n",
        "import collections.abc\n",
        "import os\n",
        "import typing\n",
        "import warnings\n",
        "from collections.abc import Sequence\n",
        "from typing import Any\n",
        "\n",
        "from onnxruntime.capi import _pybind_state as C\n",
        "\n",
        "if typing.TYPE_CHECKING:\n",
        "    import onnxruntime\n",
        "\n",
        "\n",
        "def get_ort_device_type(device_type: str, device_index) -> C.OrtDevice:\n",
        "    if device_type == \"cuda\":\n",
        "        return C.OrtDevice.cuda()\n",
        "    elif device_type == \"cann\":\n",
        "        return C.OrtDevice.cann()\n",
        "    elif device_type == \"cpu\":\n",
        "        return C.OrtDevice.cpu()\n",
        "    elif device_type == \"dml\":\n",
        "        return C.OrtDevice.dml()\n",
        "    elif device_type == \"webgpu\":\n",
        "        return C.OrtDevice.webgpu()\n",
        "    elif device_type == \"ort\":\n",
        "        return C.get_ort_device(device_index).device_type()\n",
        "    else:\n",
        "        raise Exception(\"Unsupported device type: \" + device_type)\n",
        "\n",
        "\n",
        "class AdapterFormat:\n",
        "    \"\"\"\n",
        "    This class is used to create adapter files from python structures\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, adapter=None) -> None:\n",
        "        if adapter is None:\n",
        "            self._adapter = C.AdapterFormat()\n",
        "        else:\n",
        "            self._adapter = adapter\n",
        "\n",
        "    @staticmethod\n",
        "    def read_adapter(file_path: os.PathLike) -> AdapterFormat:\n",
        "        return AdapterFormat(C.AdapterFormat.read_adapter(file_path))\n",
        "\n",
        "    def export_adapter(self, file_path: os.PathLike):\n",
        "        \"\"\"\n",
        "        This function writes a file at the specified location\n",
        "        in onnxrunitme adapter format containing Lora parameters.\n",
        "\n",
        "        :param file_path: absolute path for the adapter\n",
        "        \"\"\"\n",
        "        self._adapter.export_adapter(file_path)\n",
        "\n",
        "    def get_format_version(self):\n",
        "        return self._adapter.format_version\n",
        "\n",
        "    def set_adapter_version(self, adapter_version: int):\n",
        "        self._adapter.adapter_version = adapter_version\n",
        "\n",
        "    def get_adapter_version(self):\n",
        "        return self._adapter.adapter_version\n",
        "\n",
        "    def set_model_version(self, model_version: int):\n",
        "        self._adapter.model_version = model_version\n",
        "\n",
        "    def get_model_version(self):\n",
        "        return self._adapter.model_version\n",
        "\n",
        "    def set_parameters(self, params: dict[str, OrtValue]):\n",
        "        self._adapter.parameters = {k: v._ortvalue for k, v in params.items()}\n",
        "\n",
        "    def get_parameters(self) -> dict[str, OrtValue]:\n",
        "        return {k: OrtValue(v) for k, v in self._adapter.parameters.items()}\n",
        "\n",
        "\n",
        "def check_and_normalize_provider_args(\n",
        "    providers: Sequence[str | tuple[str, dict[Any, Any]]] | None,\n",
        "    provider_options: Sequence[dict[Any, Any]] | None,\n",
        "    available_provider_names: Sequence[str],\n",
        "):\n",
        "    \"\"\"\n",
        "    Validates the 'providers' and 'provider_options' arguments and returns a\n",
        "        normalized version.\n",
        "\n",
        "    :param providers: Optional sequence of providers in order of decreasing\n",
        "        precedence. Values can either be provider names or tuples of\n",
        "        (provider name, options dict).\n",
        "    :param provider_options: Optional sequence of options dicts corresponding\n",
        "        to the providers listed in 'providers'.\n",
        "    :param available_provider_names: The available provider names.\n",
        "\n",
        "    :return: Tuple of (normalized 'providers' sequence, normalized\n",
        "        'provider_options' sequence).\n",
        "\n",
        "    'providers' can contain either names or names and options. When any options\n",
        "        are given in 'providers', 'provider_options' should not be used.\n",
        "\n",
        "    The normalized result is a tuple of:\n",
        "    1. Sequence of provider names in the same order as 'providers'.\n",
        "    2. Sequence of corresponding provider options dicts with string keys and\n",
        "        values. Unspecified provider options yield empty dicts.\n",
        "    \"\"\"\n",
        "    if providers is None:\n",
        "        return [], []\n",
        "\n",
        "    provider_name_to_options = collections.OrderedDict()\n",
        "\n",
        "    def set_provider_options(name, options):\n",
        "        if name not in available_provider_names:\n",
        "            warnings.warn(\n",
        "                \"Specified provider '{}' is not in available provider names.Available providers: '{}'\".format(\n",
        "                    name, \", \".join(available_provider_names)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if name in provider_name_to_options:\n",
        "            warnings.warn(f\"Duplicate provider '{name}' encountered, ignoring.\")\n",
        "            return\n",
        "\n",
        "        normalized_options = {str(key): str(value) for key, value in options.items()}\n",
        "        provider_name_to_options[name] = normalized_options\n",
        "\n",
        "    if not isinstance(providers, collections.abc.Sequence):\n",
        "        raise ValueError(\"'providers' should be a sequence.\")\n",
        "\n",
        "    if provider_options is not None:\n",
        "        if not isinstance(provider_options, collections.abc.Sequence):\n",
        "            raise ValueError(\"'provider_options' should be a sequence.\")\n",
        "\n",
        "        if len(providers) != len(provider_options):\n",
        "            raise ValueError(\"'providers' and 'provider_options' should be the same length if both are given.\")\n",
        "\n",
        "        if not all(isinstance(provider, str) for provider in providers):\n",
        "            raise ValueError(\"Only string values for 'providers' are supported if 'provider_options' is given.\")\n",
        "\n",
        "        if not all(isinstance(options_for_provider, dict) for options_for_provider in provider_options):\n",
        "            raise ValueError(\"'provider_options' values must be dicts.\")\n",
        "\n",
        "        for name, options in zip(providers, provider_options, strict=False):\n",
        "            set_provider_options(name, options)\n",
        "\n",
        "    else:\n",
        "        for provider in providers:\n",
        "            if isinstance(provider, str):\n",
        "                set_provider_options(provider, {})\n",
        "            elif (\n",
        "                isinstance(provider, tuple)\n",
        "                and len(provider) == 2\n",
        "                and isinstance(provider[0], str)\n",
        "                and isinstance(provider[1], dict)\n",
        "            ):\n",
        "                set_provider_options(provider[0], provider[1])\n",
        "            else:\n",
        "                raise ValueError(\"'providers' values must be either strings or (string, dict) tuples.\")\n",
        "\n",
        "    return list(provider_name_to_options.keys()), list(provider_name_to_options.values())\n",
        "\n",
        "\n",
        "class Session:\n",
        "    \"\"\"\n",
        "    This is the main class used to run a model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # self._sess is managed by the derived class and relies on bindings from C.InferenceSession\n",
        "        self._sess = None\n",
        "        self._enable_fallback = True\n",
        "\n",
        "    def get_session_options(self):\n",
        "        \"Return the session options. See :class:`onnxruntime.SessionOptions`.\"\n",
        "        return self._sess_options\n",
        "\n",
        "    def get_inputs(self):\n",
        "        \"Return the inputs metadata as a list of :class:`onnxruntime.NodeArg`.\"\n",
        "        return self._inputs_meta\n",
        "\n",
        "    def get_outputs(self):\n",
        "        \"Return the outputs metadata as a list of :class:`onnxruntime.NodeArg`.\"\n",
        "        return self._outputs_meta\n",
        "\n",
        "    def get_overridable_initializers(self):\n",
        "        \"Return the inputs (including initializers) metadata as a list of :class:`onnxruntime.NodeArg`.\"\n",
        "        return self._overridable_initializers\n",
        "\n",
        "    def get_modelmeta(self):\n",
        "        \"Return the metadata. See :class:`onnxruntime.ModelMetadata`.\"\n",
        "        return self._model_meta\n",
        "\n",
        "    def get_providers(self):\n",
        "        \"Return list of registered execution providers.\"\n",
        "        return self._providers\n",
        "\n",
        "    def get_provider_options(self):\n",
        "        \"Return registered execution providers' configurations.\"\n",
        "        return self._provider_options\n",
        "\n",
        "    def set_providers(self, providers=None, provider_options=None):\n",
        "        \"\"\"\n",
        "        Register the input list of execution providers. The underlying session is re-created.\n",
        "\n",
        "        :param providers: Optional sequence of providers in order of decreasing\n",
        "            precedence. Values can either be provider names or tuples of\n",
        "            (provider name, options dict). If not provided, then all available\n",
        "            providers are used with the default precedence.\n",
        "        :param provider_options: Optional sequence of options dicts corresponding\n",
        "            to the providers listed in 'providers'.\n",
        "\n",
        "        'providers' can contain either names or names and options. When any options\n",
        "        are given in 'providers', 'provider_options' should not be used.\n",
        "\n",
        "        The list of providers is ordered by precedence. For example\n",
        "        `['CUDAExecutionProvider', 'CPUExecutionProvider']`\n",
        "        means execute a node using CUDAExecutionProvider if capable,\n",
        "        otherwise execute using CPUExecutionProvider.\n",
        "        \"\"\"\n",
        "        # recreate the underlying C.InferenceSession\n",
        "        self._reset_session(providers, provider_options)\n",
        "\n",
        "    def disable_fallback(self):\n",
        "        \"\"\"\n",
        "        Disable session.run() fallback mechanism.\n",
        "        \"\"\"\n",
        "        self._enable_fallback = False\n",
        "\n",
        "    def enable_fallback(self):\n",
        "        \"\"\"\n",
        "        Enable session.Run() fallback mechanism. If session.Run() fails due to an internal Execution Provider failure,\n",
        "        reset the Execution Providers enabled for this session.\n",
        "        If GPU is enabled, fall back to CUDAExecutionProvider.\n",
        "        otherwise fall back to CPUExecutionProvider.\n",
        "        \"\"\"\n",
        "        self._enable_fallback = True\n",
        "\n",
        "    def _validate_input(self, feed_input_names):\n",
        "        missing_input_names = []\n",
        "        for input in self._inputs_meta:\n",
        "            if input.name not in feed_input_names and not input.type.startswith(\"optional\"):\n",
        "                missing_input_names.append(input.name)\n",
        "        if missing_input_names:\n",
        "            raise ValueError(\n",
        "                f\"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names}).\"\n",
        "            )\n",
        "\n",
        "    def run(self, output_names, input_feed, run_options=None):\n",
        "        \"\"\"\n",
        "        Compute the predictions.\n",
        "\n",
        "        :param output_names: name of the outputs\n",
        "        :param input_feed: dictionary ``{ input_name: input_value }``\n",
        "        :param run_options: See :class:`onnxruntime.RunOptions`.\n",
        "        :return: list of results, every result is either a numpy array,\n",
        "            a sparse tensor, a list or a dictionary.\n",
        "\n",
        "        ::\n",
        "\n",
        "            sess.run([output_name], {input_name: x})\n",
        "        \"\"\"\n",
        "        self._validate_input(list(input_feed.keys()))\n",
        "        if not output_names:\n",
        "            output_names = [output.name for output in self._outputs_meta]\n",
        "        try:\n",
        "            return self._sess.run(output_names, input_feed, run_options)\n",
        "        except C.EPFail as err:\n",
        "            if self._enable_fallback:\n",
        "                print(f\"EP Error: {err!s} using {self._providers}\")\n",
        "                print(f\"Falling back to {self._fallback_providers} and retrying.\")\n",
        "                self.set_providers(self._fallback_providers)\n",
        "                # Fallback only once.\n",
        "                self.disable_fallback()\n",
        "                return self._sess.run(output_names, input_feed, run_options)\n",
        "            raise\n",
        "\n",
        "    def run_async(self, output_names, input_feed, callback, user_data, run_options=None):\n",
        "        \"\"\"\n",
        "        Compute the predictions asynchronously in a separate cxx thread from ort intra-op threadpool.\n",
        "\n",
        "        :param output_names: name of the outputs\n",
        "        :param input_feed: dictionary ``{ input_name: input_value }``\n",
        "        :param callback: python function that accept array of results, and a status string on error.\n",
        "            The callback will be invoked by a cxx thread from ort intra-op threadpool.\n",
        "        :param run_options: See :class:`onnxruntime.RunOptions`.\n",
        "\n",
        "        ::\n",
        "            class MyData:\n",
        "                def __init__(self):\n",
        "                    # ...\n",
        "                def save_results(self, results):\n",
        "                    # ...\n",
        "\n",
        "            def callback(results: np.ndarray, user_data: MyData, err: str) -> None:\n",
        "              if err:\n",
        "                 print (err)\n",
        "              else:\n",
        "                # save results to user_data\n",
        "\n",
        "            sess.run_async([output_name], {input_name: x}, callback)\n",
        "        \"\"\"\n",
        "        self._validate_input(list(input_feed.keys()))\n",
        "        if not output_names:\n",
        "            output_names = [output.name for output in self._outputs_meta]\n",
        "        return self._sess.run_async(output_names, input_feed, callback, user_data, run_options)\n",
        "\n",
        "    def run_with_ort_values(self, output_names, input_dict_ort_values, run_options=None):\n",
        "        \"\"\"\n",
        "        Compute the predictions.\n",
        "\n",
        "        :param output_names: name of the outputs\n",
        "        :param input_dict_ort_values: dictionary ``{ input_name: input_ort_value }``\n",
        "            See ``OrtValue`` class how to create `OrtValue`\n",
        "            from numpy array or `SparseTensor`\n",
        "        :param run_options: See :class:`onnxruntime.RunOptions`.\n",
        "        :return: an array of `OrtValue`\n",
        "\n",
        "        ::\n",
        "\n",
        "            sess.run([output_name], {input_name: x})\n",
        "        \"\"\"\n",
        "\n",
        "        def invoke(sess, output_names, input_dict_ort_values, run_options):\n",
        "            input_dict = {}\n",
        "            for n, v in input_dict_ort_values.items():\n",
        "                input_dict[n] = v._get_c_value()\n",
        "            result = sess.run_with_ort_values(input_dict, output_names, run_options)\n",
        "            if not isinstance(result, C.OrtValueVector):\n",
        "                raise TypeError(\"run_with_ort_values() must return a instance of type 'OrtValueVector'.\")\n",
        "            ort_values = [OrtValue(v) for v in result]\n",
        "            return ort_values\n",
        "\n",
        "        self._validate_input(list(input_dict_ort_values.keys()))\n",
        "        if not output_names:\n",
        "            output_names = [output.name for output in self._outputs_meta]\n",
        "        try:\n",
        "            return invoke(self._sess, output_names, input_dict_ort_values, run_options)\n",
        "        except C.EPFail as err:\n",
        "            if self._enable_fallback:\n",
        "                print(f\"EP Error: {err!s} using {self._providers}\")\n",
        "                print(f\"Falling back to {self._fallback_providers} and retrying.\")\n",
        "                self.set_providers(self._fallback_providers)\n",
        "                # Fallback only once.\n",
        "                self.disable_fallback()\n",
        "                return invoke(self._sess, output_names, input_dict_ort_values, run_options)\n",
        "            raise\n",
        "\n",
        "    def end_profiling(self):\n",
        "        \"\"\"\n",
        "        End profiling and return results in a file.\n",
        "\n",
        "        The results are stored in a filename if the option\n",
        "        :meth:`onnxruntime.SessionOptions.enable_profiling`.\n",
        "        \"\"\"\n",
        "        return self._sess.end_profiling()\n",
        "\n",
        "    def get_profiling_start_time_ns(self):\n",
        "        \"\"\"\n",
        "        Return the nanoseconds of profiling's start time\n",
        "        Comparable to time.monotonic_ns() after Python 3.3\n",
        "        On some platforms, this timer may not be as precise as nanoseconds\n",
        "        For instance, on Windows and MacOS, the precision will be ~100ns\n",
        "        \"\"\"\n",
        "        return self._sess.get_profiling_start_time_ns\n",
        "\n",
        "    def io_binding(self):\n",
        "        \"Return an onnxruntime.IOBinding object`.\"\n",
        "        return IOBinding(self)\n",
        "\n",
        "    def run_with_iobinding(self, iobinding, run_options=None):\n",
        "        \"\"\"\n",
        "        Compute the predictions.\n",
        "\n",
        "        :param iobinding: the iobinding object that has graph inputs/outputs bind.\n",
        "        :param run_options: See :class:`onnxruntime.RunOptions`.\n",
        "        \"\"\"\n",
        "        self._sess.run_with_iobinding(iobinding._iobinding, run_options)\n",
        "\n",
        "    def get_tuning_results(self):\n",
        "        return self._sess.get_tuning_results()\n",
        "\n",
        "    def set_tuning_results(self, results, *, error_on_invalid=False):\n",
        "        return self._sess.set_tuning_results(results, error_on_invalid)\n",
        "\n",
        "    def run_with_ortvaluevector(self, run_options, feed_names, feeds, fetch_names, fetches, fetch_devices):\n",
        "        \"\"\"\n",
        "        Compute the predictions similar to other run_*() methods but with minimal C++/Python conversion overhead.\n",
        "\n",
        "        :param run_options: See :class:`onnxruntime.RunOptions`.\n",
        "        :param feed_names: list of input names.\n",
        "        :param feeds: list of input OrtValue.\n",
        "        :param fetch_names: list of output names.\n",
        "        :param fetches: list of output OrtValue.\n",
        "        :param fetch_devices: list of output devices.\n",
        "        \"\"\"\n",
        "        self._sess.run_with_ortvaluevector(run_options, feed_names, feeds, fetch_names, fetches, fetch_devices)\n",
        "\n",
        "\n",
        "class InferenceSession(Session):\n",
        "    \"\"\"\n",
        "    This is the main class used to run a model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_or_bytes: str | bytes | os.PathLike,\n",
        "        sess_options: onnxruntime.SessionOptions | None = None,\n",
        "        providers: Sequence[str | tuple[str, dict[Any, Any]]] | None = None,\n",
        "        provider_options: Sequence[dict[Any, Any]] | None = None,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        :param path_or_bytes: Filename or serialized ONNX or ORT format model in a byte string.\n",
        "        :param sess_options: Session options.\n",
        "        :param providers: Optional sequence of providers in order of decreasing\n",
        "            precedence. Values can either be provider names or tuples of\n",
        "            (provider name, options dict). If not provided, then all available\n",
        "            providers are used with the default precedence.\n",
        "        :param provider_options: Optional sequence of options dicts corresponding\n",
        "            to the providers listed in 'providers'.\n",
        "\n",
        "        The model type will be inferred unless explicitly set in the SessionOptions.\n",
        "        To explicitly set:\n",
        "\n",
        "        ::\n",
        "\n",
        "            so = onnxruntime.SessionOptions()\n",
        "            # so.add_session_config_entry('session.load_model_format', 'ONNX') or\n",
        "            so.add_session_config_entry('session.load_model_format', 'ORT')\n",
        "\n",
        "        A file extension of '.ort' will be inferred as an ORT format model.\n",
        "        All other filenames are assumed to be ONNX format models.\n",
        "\n",
        "        'providers' can contain either names or names and options. When any options\n",
        "        are given in 'providers', 'provider_options' should not be used.\n",
        "\n",
        "        The list of providers is ordered by precedence. For example\n",
        "        `['CUDAExecutionProvider', 'CPUExecutionProvider']`\n",
        "        means execute a node using `CUDAExecutionProvider`\n",
        "        if capable, otherwise execute using `CPUExecutionProvider`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(path_or_bytes, (str, os.PathLike)):\n",
        "            self._model_path = os.fspath(path_or_bytes)\n",
        "            self._model_bytes = None\n",
        "        elif isinstance(path_or_bytes, bytes):\n",
        "            self._model_path = None\n",
        "            self._model_bytes = path_or_bytes  # TODO: This is bad as we're holding the memory indefinitely\n",
        "        else:\n",
        "            raise TypeError(f\"Unable to load from type '{type(path_or_bytes)}'\")\n",
        "\n",
        "        self._sess_options = sess_options\n",
        "        self._sess_options_initial = sess_options\n",
        "        self._enable_fallback = True\n",
        "        if \"read_config_from_model\" in kwargs:\n",
        "            self._read_config_from_model = int(kwargs[\"read_config_from_model\"]) == 1\n",
        "        else:\n",
        "            self._read_config_from_model = os.environ.get(\"ORT_LOAD_CONFIG_FROM_MODEL\") == \"1\"\n",
        "\n",
        "        # internal parameters that we don't expect to be used in general so aren't documented\n",
        "        disabled_optimizers = kwargs.get(\"disabled_optimizers\")\n",
        "\n",
        "        try:\n",
        "            self._create_inference_session(providers, provider_options, disabled_optimizers)\n",
        "        except (ValueError, RuntimeError) as e:\n",
        "            if self._enable_fallback:\n",
        "                try:\n",
        "                    print(\"*************** EP Error ***************\")\n",
        "                    print(f\"EP Error {e} when using {providers}\")\n",
        "                    print(f\"Falling back to {self._fallback_providers} and retrying.\")\n",
        "                    print(\"****************************************\")\n",
        "                    self._create_inference_session(self._fallback_providers, None)\n",
        "                    # Fallback only once.\n",
        "                    self.disable_fallback()\n",
        "                    return\n",
        "                except Exception as fallback_error:\n",
        "                    raise fallback_error from e\n",
        "            # Fallback is disabled. Raise the original error.\n",
        "            raise e\n",
        "\n",
        "    def _create_inference_session(self, providers, provider_options, disabled_optimizers=None):\n",
        "        available_providers = C.get_available_providers()\n",
        "\n",
        "        # Tensorrt can fall back to CUDA if it's explicitly assigned. All others fall back to CPU.\n",
        "        if \"TensorrtExecutionProvider\" in available_providers:\n",
        "            if (\n",
        "                providers\n",
        "                and any(\n",
        "                    provider == \"CUDAExecutionProvider\"\n",
        "                    or (isinstance(provider, tuple) and provider[0] == \"CUDAExecutionProvider\")\n",
        "                    for provider in providers\n",
        "                )\n",
        "                and any(\n",
        "                    provider == \"TensorrtExecutionProvider\"\n",
        "                    or (isinstance(provider, tuple) and provider[0] == \"TensorrtExecutionProvider\")\n",
        "                    for provider in providers\n",
        "                )\n",
        "            ):\n",
        "                self._fallback_providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        "            else:\n",
        "                self._fallback_providers = [\"CPUExecutionProvider\"]\n",
        "        # MIGraphX can fall back to ROCM if it's explicitly assigned. All others fall back to CPU.\n",
        "        elif \"MIGraphXExecutionProvider\" in available_providers:\n",
        "            if providers and any(\n",
        "                provider == \"ROCMExecutionProvider\"\n",
        "                or (isinstance(provider, tuple) and provider[0] == \"ROCMExecutionProvider\")\n",
        "                for provider in providers\n",
        "            ):\n",
        "                self._fallback_providers = [\"ROCMExecutionProvider\", \"CPUExecutionProvider\"]\n",
        "            else:\n",
        "                self._fallback_providers = [\"CPUExecutionProvider\"]\n",
        "        else:\n",
        "            self._fallback_providers = [\"CPUExecutionProvider\"]\n",
        "\n",
        "        # validate providers and provider_options before other initialization\n",
        "        providers, provider_options = check_and_normalize_provider_args(\n",
        "            providers, provider_options, available_providers\n",
        "        )\n",
        "\n",
        "        session_options = self._sess_options if self._sess_options else C.get_default_session_options()\n",
        "\n",
        "        self._register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n",
        "\n",
        "        if self._model_path:\n",
        "            sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)\n",
        "        else:\n",
        "            sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)\n",
        "\n",
        "        if disabled_optimizers is None:\n",
        "            disabled_optimizers = set()\n",
        "        elif not isinstance(disabled_optimizers, set):\n",
        "            # convert to set. assumes iterable\n",
        "            disabled_optimizers = set(disabled_optimizers)\n",
        "\n",
        "        # initialize the C++ InferenceSession\n",
        "        sess.initialize_session(providers, provider_options, disabled_optimizers)\n",
        "\n",
        "        self._sess = sess\n",
        "        self._sess_options = self._sess.session_options\n",
        "        self._inputs_meta = self._sess.inputs_meta\n",
        "        self._outputs_meta = self._sess.outputs_meta\n",
        "        self._overridable_initializers = self._sess.overridable_initializers\n",
        "        self._model_meta = self._sess.model_meta\n",
        "        self._providers = self._sess.get_providers()\n",
        "        self._provider_options = self._sess.get_provider_options()\n",
        "        self._profiling_start_time_ns = self._sess.get_profiling_start_time_ns\n",
        "\n",
        "    def _reset_session(self, providers, provider_options):\n",
        "        \"release underlying session object.\"\n",
        "        # meta data references session internal structures\n",
        "        # so they must be set to None to decrement _sess reference count.\n",
        "        self._sess_options = None\n",
        "        self._inputs_meta = None\n",
        "        self._outputs_meta = None\n",
        "        self._overridable_initializers = None\n",
        "        self._model_meta = None\n",
        "        self._providers = None\n",
        "        self._provider_options = None\n",
        "        self._profiling_start_time_ns = None\n",
        "\n",
        "        # create a new C.InferenceSession\n",
        "        self._sess = None\n",
        "        self._sess_options = self._sess_options_initial\n",
        "        self._create_inference_session(providers, provider_options)\n",
        "\n",
        "    def _register_ep_custom_ops(self, session_options, providers, provider_options, available_providers):\n",
        "        for i in range(len(providers)):\n",
        "            if providers[i] in available_providers and providers[i] == \"TensorrtExecutionProvider\":\n",
        "                C.register_tensorrt_plugins_as_custom_ops(session_options, provider_options[i])\n",
        "            elif (\n",
        "                isinstance(providers[i], tuple)\n",
        "                and providers[i][0] in available_providers\n",
        "                and providers[i][0] == \"TensorrtExecutionProvider\"\n",
        "            ):\n",
        "                C.register_tensorrt_plugins_as_custom_ops(session_options, providers[i][1])\n",
        "\n",
        "\n",
        "class IOBinding:\n",
        "    \"\"\"\n",
        "    This class provides API to bind input/output to a specified device, e.g. GPU.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, session: Session):\n",
        "        self._iobinding = C.SessionIOBinding(session._sess)\n",
        "        self._numpy_obj_references = {}\n",
        "\n",
        "    def bind_cpu_input(self, name, arr_on_cpu):\n",
        "        \"\"\"\n",
        "        bind an input to array on CPU\n",
        "        :param name: input name\n",
        "        :param arr_on_cpu: input values as a python array on CPU\n",
        "        \"\"\"\n",
        "        # Hold a reference to the numpy object as the bound OrtValue is backed\n",
        "        # directly by the data buffer of the numpy object and so the numpy object\n",
        "        # must be around until this IOBinding instance is around\n",
        "        self._numpy_obj_references[name] = arr_on_cpu\n",
        "        self._iobinding.bind_input(name, arr_on_cpu)\n",
        "\n",
        "    def bind_input(self, name, device_type, device_id, element_type, shape, buffer_ptr):\n",
        "        \"\"\"\n",
        "        :param name: input name\n",
        "        :param device_type: e.g. cpu, cuda, cann\n",
        "        :param device_id: device id, e.g. 0\n",
        "        :param element_type: input element type. It can be either numpy type (like numpy.float32) or an integer for onnx type (like onnx.TensorProto.BFLOAT16)\n",
        "        :param shape: input shape\n",
        "        :param buffer_ptr: memory pointer to input data\n",
        "        \"\"\"\n",
        "        self._iobinding.bind_input(\n",
        "            name,\n",
        "            C.OrtDevice(\n",
        "                get_ort_device_type(device_type, device_id),\n",
        "                C.OrtDevice.default_memory(),\n",
        "                device_id,\n",
        "            ),\n",
        "            element_type,\n",
        "            shape,\n",
        "            buffer_ptr,\n",
        "        )\n",
        "\n",
        "    def bind_ortvalue_input(self, name, ortvalue):\n",
        "        \"\"\"\n",
        "        :param name: input name\n",
        "        :param ortvalue: OrtValue instance to bind\n",
        "        \"\"\"\n",
        "        self._iobinding.bind_ortvalue_input(name, ortvalue._ortvalue)\n",
        "\n",
        "    def synchronize_inputs(self):\n",
        "        self._iobinding.synchronize_inputs()\n",
        "\n",
        "    def bind_output(\n",
        "        self,\n",
        "        name,\n",
        "        device_type=\"cpu\",\n",
        "        device_id=0,\n",
        "        element_type=None,\n",
        "        shape=None,\n",
        "        buffer_ptr=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param name: output name\n",
        "        :param device_type: e.g. cpu, cuda, cann, cpu by default\n",
        "        :param device_id: device id, e.g. 0\n",
        "        :param element_type: output element type. It can be either numpy type (like numpy.float32) or an integer for onnx type (like onnx.TensorProto.BFLOAT16)\n",
        "        :param shape: output shape\n",
        "        :param buffer_ptr: memory pointer to output data\n",
        "        \"\"\"\n",
        "\n",
        "        # Follow the `if` path when the user has not provided any pre-allocated buffer but still\n",
        "        # would like to bind an output to a specific device (e.g. cuda).\n",
        "        # Pre-allocating an output buffer may not be an option for the user as :\n",
        "        # (1) They may not want to use a custom allocator specific to the device they want to bind the output to,\n",
        "        # in which case ORT will allocate the memory for the user\n",
        "        # (2) The output has a dynamic shape and hence the size of the buffer may not be fixed across runs\n",
        "        if buffer_ptr is None:\n",
        "            self._iobinding.bind_output(\n",
        "                name,\n",
        "                C.OrtDevice(\n",
        "                    get_ort_device_type(device_type, device_id),\n",
        "                    C.OrtDevice.default_memory(),\n",
        "                    device_id,\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            if element_type is None or shape is None:\n",
        "                raise ValueError(\"`element_type` and `shape` are to be provided if pre-allocated memory is provided\")\n",
        "            self._iobinding.bind_output(\n",
        "                name,\n",
        "                C.OrtDevice(\n",
        "                    get_ort_device_type(device_type, device_id),\n",
        "                    C.OrtDevice.default_memory(),\n",
        "                    device_id,\n",
        "                ),\n",
        "                element_type,\n",
        "                shape,\n",
        "                buffer_ptr,\n",
        "            )\n",
        "\n",
        "    def bind_ortvalue_output(self, name, ortvalue):\n",
        "        \"\"\"\n",
        "        :param name: output name\n",
        "        :param ortvalue: OrtValue instance to bind\n",
        "        \"\"\"\n",
        "        self._iobinding.bind_ortvalue_output(name, ortvalue._ortvalue)\n",
        "\n",
        "    def synchronize_outputs(self):\n",
        "        self._iobinding.synchronize_outputs()\n",
        "\n",
        "    def get_outputs(self):\n",
        "        \"\"\"\n",
        "        Returns the output OrtValues from the Run() that preceded the call.\n",
        "        The data buffer of the obtained OrtValues may not reside on CPU memory\n",
        "        \"\"\"\n",
        "        outputs = self._iobinding.get_outputs()\n",
        "        if not isinstance(outputs, C.OrtValueVector):\n",
        "            raise TypeError(\"get_outputs() must return an instance of type 'OrtValueVector'.\")\n",
        "        return [OrtValue(ortvalue) for ortvalue in outputs]\n",
        "\n",
        "    def get_outputs_as_ortvaluevector(self):\n",
        "        return self._iobinding.get_outputs()\n",
        "\n",
        "    def copy_outputs_to_cpu(self):\n",
        "        \"\"\"Copy output contents to CPU.\"\"\"\n",
        "        return self._iobinding.copy_outputs_to_cpu()\n",
        "\n",
        "    def clear_binding_inputs(self):\n",
        "        self._iobinding.clear_binding_inputs()\n",
        "\n",
        "    def clear_binding_outputs(self):\n",
        "        self._iobinding.clear_binding_outputs()\n",
        "\n",
        "\n",
        "class OrtValue:\n",
        "    \"\"\"\n",
        "    A data structure that supports all ONNX data formats (tensors and non-tensors) that allows users\n",
        "    to place the data backing these on a device, for example, on a CUDA supported device.\n",
        "    This class provides APIs to construct and deal with OrtValues.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ortvalue, numpy_obj=None):\n",
        "        if isinstance(ortvalue, C.OrtValue):\n",
        "            self._ortvalue = ortvalue\n",
        "            # Hold a ref count to the numpy object if the OrtValue is backed directly\n",
        "            # by its data buffer so that it isn't destroyed when the OrtValue is in use\n",
        "            self._numpy_obj = numpy_obj\n",
        "        else:\n",
        "            # An end user won't hit this error\n",
        "            raise ValueError(\n",
        "                \"`Provided ortvalue` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.OrtValue`\"\n",
        "            )\n",
        "\n",
        "    def _get_c_value(self):\n",
        "        return self._ortvalue\n",
        "\n",
        "    @staticmethod\n",
        "    def ortvalue_from_numpy(numpy_obj, device_type=\"cpu\", device_id=0):\n",
        "        \"\"\"\n",
        "        Factory method to construct an OrtValue (which holds a Tensor) from a given Numpy object\n",
        "        A copy of the data in the Numpy object is held by the OrtValue only if the device is NOT cpu\n",
        "\n",
        "        :param numpy_obj: The Numpy object to construct the OrtValue from\n",
        "        :param device_type: e.g. cpu, cuda, cann, cpu by default\n",
        "        :param device_id: device id, e.g. 0\n",
        "        \"\"\"\n",
        "        # Hold a reference to the numpy object (if device_type is 'cpu') as the OrtValue\n",
        "        # is backed directly by the data buffer of the numpy object and so the numpy object\n",
        "        # must be around until this OrtValue instance is around\n",
        "        return OrtValue(\n",
        "            C.OrtValue.ortvalue_from_numpy(\n",
        "                numpy_obj,\n",
        "                C.OrtDevice(\n",
        "                    get_ort_device_type(device_type, device_id),\n",
        "                    C.OrtDevice.default_memory(),\n",
        "                    device_id,\n",
        "                ),\n",
        "            ),\n",
        "            numpy_obj if device_type.lower() == \"cpu\" else None,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def ortvalue_from_numpy_with_onnx_type(data, onnx_element_type: int):\n",
        "        \"\"\"\n",
        "        This method creates an instance of OrtValue on top of the numpy array.\n",
        "        No data copy is made and the lifespan of the resulting OrtValue should never\n",
        "        exceed the lifespan of bytes object. The API attempts to reinterpret\n",
        "        the data type which is expected to be the same size. This is useful\n",
        "        when we want to use an ONNX data type that is not supported by numpy.\n",
        "\n",
        "        :param data: numpy.ndarray.\n",
        "        :param onnx_elemenet_type: a valid onnx TensorProto::DataType enum value\n",
        "        \"\"\"\n",
        "        return OrtValue(C.OrtValue.ortvalue_from_numpy_with_onnx_type(data, onnx_element_type), data)\n",
        "\n",
        "    @staticmethod\n",
        "    def ortvalue_from_shape_and_type(shape, element_type, device_type: str = \"cpu\", device_id: int = 0):\n",
        "        \"\"\"\n",
        "        Factory method to construct an OrtValue (which holds a Tensor) from given shape and element_type\n",
        "\n",
        "        :param shape: List of integers indicating the shape of the OrtValue\n",
        "        :param element_type: The data type of the elements. It can be either numpy type (like numpy.float32) or an integer for onnx type (like onnx.TensorProto.BFLOAT16).\n",
        "        :param device_type: e.g. cpu, cuda, cann, cpu by default\n",
        "        :param device_id: device id, e.g. 0\n",
        "        \"\"\"\n",
        "        # Integer for onnx element type (see https://onnx.ai/onnx/api/mapping.html).\n",
        "        # This is helpful for some data type (like TensorProto.BFLOAT16) that is not available in numpy.\n",
        "        if isinstance(element_type, int):\n",
        "            return OrtValue(\n",
        "                C.OrtValue.ortvalue_from_shape_and_onnx_type(\n",
        "                    shape,\n",
        "                    element_type,\n",
        "                    C.OrtDevice(\n",
        "                        get_ort_device_type(device_type, device_id),\n",
        "                        C.OrtDevice.default_memory(),\n",
        "                        device_id,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return OrtValue(\n",
        "            C.OrtValue.ortvalue_from_shape_and_type(\n",
        "                shape,\n",
        "                element_type,\n",
        "                C.OrtDevice(\n",
        "                    get_ort_device_type(device_type, device_id),\n",
        "                    C.OrtDevice.default_memory(),\n",
        "                    device_id,\n",
        "                ),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def ort_value_from_sparse_tensor(sparse_tensor):\n",
        "        \"\"\"\n",
        "        The function will construct an OrtValue instance from a valid SparseTensor\n",
        "        The new instance of OrtValue will assume the ownership of sparse_tensor\n",
        "        \"\"\"\n",
        "        return OrtValue(C.OrtValue.ort_value_from_sparse_tensor(sparse_tensor._get_c_tensor()))\n",
        "\n",
        "    def as_sparse_tensor(self):\n",
        "        \"\"\"\n",
        "        The function will return SparseTensor contained in this OrtValue\n",
        "        \"\"\"\n",
        "        return SparseTensor(self._ortvalue.as_sparse_tensor())\n",
        "\n",
        "    def data_ptr(self):\n",
        "        \"\"\"\n",
        "        Returns the address of the first element in the OrtValue's data buffer\n",
        "        \"\"\"\n",
        "        return self._ortvalue.data_ptr()\n",
        "\n",
        "    def device_name(self):\n",
        "        \"\"\"\n",
        "        Returns the name of the device where the OrtValue's data buffer resides e.g. cpu, cuda, cann\n",
        "        \"\"\"\n",
        "        return self._ortvalue.device_name().lower()\n",
        "\n",
        "    def shape(self):\n",
        "        \"\"\"\n",
        "        Returns the shape of the data in the OrtValue\n",
        "        \"\"\"\n",
        "        return self._ortvalue.shape()\n",
        "\n",
        "    def data_type(self):\n",
        "        \"\"\"\n",
        "        Returns the data type of the data in the OrtValue\n",
        "        \"\"\"\n",
        "        return self._ortvalue.data_type()\n",
        "\n",
        "    def element_type(self):\n",
        "        \"\"\"\n",
        "        Returns the proto type of the data in the OrtValue\n",
        "        if the OrtValue is a tensor.\n",
        "        \"\"\"\n",
        "        return self._ortvalue.element_type()\n",
        "\n",
        "    def has_value(self):\n",
        "        \"\"\"\n",
        "        Returns True if the OrtValue corresponding to an\n",
        "        optional type contains data, else returns False\n",
        "        \"\"\"\n",
        "        return self._ortvalue.has_value()\n",
        "\n",
        "    def is_tensor(self):\n",
        "        \"\"\"\n",
        "        Returns True if the OrtValue contains a Tensor, else returns False\n",
        "        \"\"\"\n",
        "        return self._ortvalue.is_tensor()\n",
        "\n",
        "    def is_sparse_tensor(self):\n",
        "        \"\"\"\n",
        "        Returns True if the OrtValue contains a SparseTensor, else returns False\n",
        "        \"\"\"\n",
        "        return self._ortvalue.is_sparse_tensor()\n",
        "\n",
        "    def is_tensor_sequence(self):\n",
        "        \"\"\"\n",
        "        Returns True if the OrtValue contains a Tensor Sequence, else returns False\n",
        "        \"\"\"\n",
        "        return self._ortvalue.is_tensor_sequence()\n",
        "\n",
        "    def numpy(self):\n",
        "        \"\"\"\n",
        "        Returns a Numpy object from the OrtValue.\n",
        "        Valid only for OrtValues holding Tensors. Throws for OrtValues holding non-Tensors.\n",
        "        Use accessors to gain a reference to non-Tensor objects such as SparseTensor\n",
        "        \"\"\"\n",
        "        return self._ortvalue.numpy()\n",
        "\n",
        "    def update_inplace(self, np_arr):\n",
        "        \"\"\"\n",
        "        Update the OrtValue in place with a new Numpy array. The numpy contents\n",
        "        are copied over to the device memory backing the OrtValue. It can be used\n",
        "        to update the input valuess for an InferenceSession with CUDA graph\n",
        "        enabled or other scenarios where the OrtValue needs to be updated while\n",
        "        the memory address can not be changed.\n",
        "        \"\"\"\n",
        "        self._ortvalue.update_inplace(np_arr)\n",
        "\n",
        "\n",
        "class OrtDevice:\n",
        "    \"\"\"\n",
        "    A data structure that exposes the underlying C++ OrtDevice\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, c_ort_device):\n",
        "        \"\"\"\n",
        "        Internal constructor\n",
        "        \"\"\"\n",
        "        if isinstance(c_ort_device, C.OrtDevice):\n",
        "            self._ort_device = c_ort_device\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"`Provided object` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.OrtDevice`\"\n",
        "            )\n",
        "\n",
        "    def _get_c_device(self):\n",
        "        \"\"\"\n",
        "        Internal accessor to underlying object\n",
        "        \"\"\"\n",
        "        return self._ort_device\n",
        "\n",
        "    @staticmethod\n",
        "    def make(ort_device_name, device_id):\n",
        "        return OrtDevice(\n",
        "            C.OrtDevice(\n",
        "                get_ort_device_type(ort_device_name, device_id),\n",
        "                C.OrtDevice.default_memory(),\n",
        "                device_id,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def device_id(self):\n",
        "        return self._ort_device.device_id()\n",
        "\n",
        "    def device_type(self):\n",
        "        return self._ort_device.device_type()\n",
        "\n",
        "\n",
        "class SparseTensor:\n",
        "    \"\"\"\n",
        "    A data structure that project the C++ SparseTensor object\n",
        "    The class provides API to work with the object.\n",
        "    Depending on the format, the class will hold more than one buffer\n",
        "    depending on the format\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sparse_tensor):\n",
        "        \"\"\"\n",
        "        Internal constructor\n",
        "        \"\"\"\n",
        "        if isinstance(sparse_tensor, C.SparseTensor):\n",
        "            self._tensor = sparse_tensor\n",
        "        else:\n",
        "            # An end user won't hit this error\n",
        "            raise ValueError(\n",
        "                \"`Provided object` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.SparseTensor`\"\n",
        "            )\n",
        "\n",
        "    def _get_c_tensor(self):\n",
        "        return self._tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def sparse_coo_from_numpy(dense_shape, values, coo_indices, ort_device):\n",
        "        \"\"\"\n",
        "        Factory method to construct a SparseTensor in COO format from given arguments\n",
        "\n",
        "        :param dense_shape: 1-D  numpy array(int64) or a python list that contains a dense_shape of the sparse tensor\n",
        "            must be on cpu memory\n",
        "        :param values: a homogeneous, contiguous 1-D numpy array that contains non-zero elements of the tensor\n",
        "            of a type.\n",
        "        :param coo_indices:  contiguous numpy array(int64) that contains COO indices for the tensor. coo_indices may\n",
        "            have a 1-D shape when it contains a linear index of non-zero values and its length must be equal to\n",
        "            that of the values. It can also be of 2-D shape, in which has it contains pairs of coordinates for\n",
        "            each of the nnz values and its length must be exactly twice of the values length.\n",
        "        :param ort_device: - describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is\n",
        "            suppored for non-numeric data types.\n",
        "\n",
        "        For primitive types, the method will map values and coo_indices arrays into native memory and will use\n",
        "        them as backing storage. It will increment the reference count for numpy arrays and it will decrement it\n",
        "        on GC. The buffers may reside in any storage either CPU or GPU.\n",
        "        For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those\n",
        "        on other devices and their memory can not be mapped.\n",
        "        \"\"\"\n",
        "        return SparseTensor(\n",
        "            C.SparseTensor.sparse_coo_from_numpy(dense_shape, values, coo_indices, ort_device._get_c_device())\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def sparse_csr_from_numpy(dense_shape, values, inner_indices, outer_indices, ort_device):\n",
        "        \"\"\"\n",
        "        Factory method to construct a SparseTensor in CSR format from given arguments\n",
        "\n",
        "        :param dense_shape: 1-D numpy array(int64) or a python list that contains a dense_shape of the\n",
        "            sparse tensor (rows, cols) must be on cpu memory\n",
        "        :param values: a  contiguous, homogeneous 1-D numpy array that contains non-zero elements of the tensor\n",
        "            of a type.\n",
        "        :param inner_indices:  contiguous 1-D numpy array(int64) that contains CSR inner indices for the tensor.\n",
        "            Its length must be equal to that of the values.\n",
        "        :param outer_indices:  contiguous 1-D numpy array(int64) that contains CSR outer indices for the tensor.\n",
        "            Its length must be equal to the number of rows + 1.\n",
        "        :param ort_device: - describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is\n",
        "            suppored for non-numeric data types.\n",
        "\n",
        "        For primitive types, the method will map values and indices arrays into native memory and will use them as\n",
        "        backing storage. It will increment the reference count and it will decrement then count when it is GCed.\n",
        "        The buffers may reside in any storage either CPU or GPU.\n",
        "        For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those\n",
        "        on other devices and their memory can not be mapped.\n",
        "        \"\"\"\n",
        "        return SparseTensor(\n",
        "            C.SparseTensor.sparse_csr_from_numpy(\n",
        "                dense_shape,\n",
        "                values,\n",
        "                inner_indices,\n",
        "                outer_indices,\n",
        "                ort_device._get_c_device(),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def values(self):\n",
        "        \"\"\"\n",
        "        The method returns a numpy array that is backed by the native memory\n",
        "        if the data type is numeric. Otherwise, the returned numpy array that contains\n",
        "        copies of the strings.\n",
        "        \"\"\"\n",
        "        return self._tensor.values()\n",
        "\n",
        "    def as_coo_view(self):\n",
        "        \"\"\"\n",
        "        The method will return coo representation of the sparse tensor which will enable\n",
        "        querying COO indices. If the instance did not contain COO format, it would throw.\n",
        "        You can query coo indices as:\n",
        "\n",
        "        ::\n",
        "\n",
        "            coo_indices = sparse_tensor.as_coo_view().indices()\n",
        "\n",
        "        which will return a numpy array that is backed by the native memory.\n",
        "        \"\"\"\n",
        "        return self._tensor.get_coo_data()\n",
        "\n",
        "    def as_csrc_view(self):\n",
        "        \"\"\"\n",
        "        The method will return CSR(C) representation of the sparse tensor which will enable\n",
        "        querying CRS(C) indices. If the instance dit not contain CSR(C) format, it would throw.\n",
        "        You can query indices as:\n",
        "\n",
        "        ::\n",
        "\n",
        "            inner_ndices = sparse_tensor.as_csrc_view().inner()\n",
        "            outer_ndices = sparse_tensor.as_csrc_view().outer()\n",
        "\n",
        "        returning numpy arrays backed by the native memory.\n",
        "        \"\"\"\n",
        "        return self._tensor.get_csrc_data()\n",
        "\n",
        "    def as_blocksparse_view(self):\n",
        "        \"\"\"\n",
        "        The method will return coo representation of the sparse tensor which will enable\n",
        "        querying BlockSparse indices. If the instance did not contain BlockSparse format, it would throw.\n",
        "        You can query coo indices as:\n",
        "\n",
        "        ::\n",
        "\n",
        "            block_sparse_indices = sparse_tensor.as_blocksparse_view().indices()\n",
        "\n",
        "        which will return a numpy array that is backed by the native memory\n",
        "        \"\"\"\n",
        "        return self._tensor.get_blocksparse_data()\n",
        "\n",
        "    def to_cuda(self, ort_device):\n",
        "        \"\"\"\n",
        "        Returns a copy of this instance on the specified cuda device\n",
        "\n",
        "        :param ort_device: with name 'cuda' and valid gpu device id\n",
        "\n",
        "        The method will throw if:\n",
        "\n",
        "        - this instance contains strings\n",
        "        - this instance is already on GPU. Cross GPU copy is not supported\n",
        "        - CUDA is not present in this build\n",
        "        - if the specified device is not valid\n",
        "        \"\"\"\n",
        "        return SparseTensor(self._tensor.to_cuda(ort_device._get_c_device()))\n",
        "\n",
        "    def format(self):\n",
        "        \"\"\"\n",
        "        Returns a OrtSparseFormat enumeration\n",
        "        \"\"\"\n",
        "        return self._tensor.format\n",
        "\n",
        "    def dense_shape(self):\n",
        "        \"\"\"\n",
        "        Returns a numpy array(int64) containing a dense shape of a sparse tensor\n",
        "        \"\"\"\n",
        "        return self._tensor.dense_shape()\n",
        "\n",
        "    def data_type(self):\n",
        "        \"\"\"\n",
        "        Returns a string data type of the data in the OrtValue\n",
        "        \"\"\"\n",
        "        return self._tensor.data_type()\n",
        "\n",
        "    def device_name(self):\n",
        "        \"\"\"\n",
        "        Returns the name of the device where the SparseTensor data buffers reside e.g. cpu, cuda\n",
        "        \"\"\"\n",
        "        return self._tensor.device_name().lower()"
      ],
      "metadata": {
        "id": "w4uv4GFtzOxQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/1.py -m onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA"
      ],
      "metadata": {
        "id": "ErX66bbwzPOS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate a pipeline for sentiment-analysis\n",
        "const pipe = await pipeline('sentiment-analysis' , { dtype: \"fp32\" });\n",
        "\n",
        "const out = await pipe('I love transformers!');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]"
      ],
      "metadata": {
        "id": "5i_0DjC8zZav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate a pipeline for sentiment-analysis\n",
        "const pipe = await pipeline('sentiment-analysis', 'nlptown/bert-base-multilingual-uncased-sentiment' , { dtype: \"fp32\" });\n",
        "\n",
        "const out = await pipe('I love transformers!');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W_iic59w0mpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\n",
        "  { dtype: \"q4f16\" },\n",
        ");"
      ],
      "metadata": {
        "id": "feE6LPp00SeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/2.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVZBPRlG0FpS",
        "outputId": "c8f8d049-13e8-4b89-b77a-ee43807a805e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@huggingface/transformers/dist/transformers.mjs:31507\n",
            "            part = part.replace(new RegExp('/$'), '');\n",
            "                        ^\n",
            "\n",
            "TypeError: part.replace is not a function\n",
            "    at file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31507:25\n",
            "    at Array.map (<anonymous>)\n",
            "    at pathJoin (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31502:19)\n",
            "    at getModelFile (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31248:22)\n",
            "    at getModelJSON (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31427:24)\n",
            "    at loadTokenizer (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24732:68)\n",
            "    at AutoTokenizer.from_pretrained (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:29020:56)\n",
            "    at loadItems (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24594:27)\n",
            "    at pipeline (file:///content/node_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24532:27)\n",
            "    at file:///content/2.js:4:20\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSsBcBD90IFg",
        "outputId": "0f2d7ff1-05d8-4e9f-fa49-d1bce1c5ee81"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBefpgde1Y6L",
        "outputId": "9c965154-9df5-4621-d926-2d88b0832f67"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime-genai in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-genai) (2.2.2)\n",
            "Requirement already satisfied: onnxruntime>=1.20.1 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-genai) (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->onnxruntime-genai) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.20.1->onnxruntime-genai) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.20.1->onnxruntime-genai) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers.js/index\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/huggingface/optimum#onnx--onnx-runtime"
      ],
      "metadata": {
        "id": "RsO69oBg1i0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum[exporters,onnxruntime]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AWbziF81f1Z",
        "outputId": "9e5bbb58-5df3-4e64-c4a2-2a9e197ddb22"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optimum[exporters,onnxruntime] in /usr/local/lib/python3.11/dist-packages (1.24.0)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (0.28.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (1.17.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (1.20.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (1.0.14)\n",
            "Requirement already satisfied: datasets>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (3.2.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (0.4.3)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters,onnxruntime]) (5.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (19.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=1.2.1->optimum[exporters,onnxruntime]) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (3.11.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[exporters,onnxruntime]) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum[exporters,onnxruntime]) (4.12.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime->optimum[exporters,onnxruntime]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->optimum[exporters,onnxruntime]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime->optimum[exporters,onnxruntime]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters,onnxruntime]) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime->optimum[exporters,onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[exporters,onnxruntime]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[exporters,onnxruntime]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[exporters,onnxruntime]) (0.5.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->optimum[exporters,onnxruntime]) (0.21.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[exporters,onnxruntime]) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[exporters,onnxruntime]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[exporters,onnxruntime]) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[exporters,onnxruntime]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[exporters,onnxruntime]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[exporters,onnxruntime]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[exporters,onnxruntime]) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[exporters,onnxruntime]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[exporters,onnxruntime]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[exporters,onnxruntime]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[exporters,onnxruntime]) (2025.1.31)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime->optimum[exporters,onnxruntime]) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[exporters,onnxruntime]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[exporters,onnxruntime]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[exporters,onnxruntime]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[exporters,onnxruntime]) (2025.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm->optimum[exporters,onnxruntime]) (11.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.2.1->optimum[exporters,onnxruntime]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export onnx -m deepset/roberta-base-squad2 --optimize O2 roberta_base_qa_onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMFFcaPE1gcQ",
        "outputId": "85904287-0df6-4514-f534-6f78bec6d9f8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-11 21:07:33.015951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739308053.426632   25675 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739308053.534226   25675 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "config.json: 100% 571/571 [00:00<00:00, 2.24MB/s]\n",
            "model.safetensors: 100% 496M/496M [00:09<00:00, 54.5MB/s]\n",
            "tokenizer_config.json: 100% 79.0/79.0 [00:00<00:00, 439kB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 1.69MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 29.8MB/s]\n",
            "special_tokens_map.json: 100% 772/772 [00:00<00:00, 4.39MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/configuration.py:784: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
            "  warnings.warn(\n",
            "symbolic shape inference disabled or failed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate a pipeline for sentiment-analysis\n",
        "const pipe = await pipeline('sentiment-analysis');\n",
        "\n",
        "const out = await pipe('I love transformers!');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]"
      ],
      "metadata": {
        "id": "Ik3xjwvi1oUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node 2.js\n",
        "Colab paid products - Cancel contracts here\n",
        "\n",
        "  2s\n",
        "completed at 1:11 PM\n",
        "pip install transformers\n",
        "Run cell (Ctrl+"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKsnejLo1xvh",
        "outputId": "07feec2f-bb88-4e48-a495-5304ead7bb9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype not specified for \"model\". Using the default dtype (fp32) for this device (cpu).\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!npm install @huggingface/transformers\n",
        "!pip install onnxruntime\n",
        "!npm install onnxruntime-node\n",
        "!npm install onnxruntime-web"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMqPzSpG28Os",
        "outputId": "5650b4dc-23b5-42dd-93b8-d15e70fcb40b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 83 packages in 3s\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K20 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0KRequirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "up to date, audited 83 packages in 2s\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K20 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 83 packages in 2s\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K20 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "source": [
        "//import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate pipeline\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX', {\n",
        "  device: 'webgpu', // Setting WebGPU as the device\n",
        "});"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Wrts_6992-W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "js\n",
        "// Install necessary libraries\n",
        "!npm install @huggingface/transformers onnxruntime-web\n",
        "\n",
        "// Create sentiment analysis pipeline with WebGPU\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', {\n",
        "  device: 'webgpu',\n",
        "});\n",
        "\n",
        "// Run the model\n",
        "const out = await pipe('I love transformers!');\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hZ7_5bHn3EvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8OsczRT3fdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A712X13s3fZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال ع المعالج"
      ],
      "metadata": {
        "id": "53_zJS_U3kGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#!node /content/2.js\n",
        "\n",
        "// Create sentiment analysis pipeline with WebGPU\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', {\n",
        "  device: 'cpu',\n",
        "});\n",
        "\n",
        "// Run the model\n",
        "const out = await pipe('I love transformers!');\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "metadata": {
        "id": "QuBdSjHs3fWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/2.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPa9HSqF3NjJ",
        "outputId": "77b6e74b-a908-4336-831d-a670924d3ea8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype not specified for \"model\". Using the default dtype (fp32) for this device (cpu).\n",
            "[ { label: \u001b[32m'POSITIVE'\u001b[39m, score: \u001b[33m0.9998068809509277\u001b[39m } ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nENmBZru3Rrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qoTW-0tY3paQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ny_mWyTI3pXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZHp12NCR3pVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bYm7t8M3pSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#!node /content/2.js\n",
        "\n",
        "\n",
        "\n",
        "// Create sentiment analysis pipeline with WebGPU\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  device: 'cpu',\n",
        "});\n",
        "\n",
        "// Run the model\n",
        "const out = await pipe('who is ai?');\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "metadata": {
        "id": "c95IsmPo3pO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "\n",
        "// Create text generation pipeline with WebGPU\n",
        "import { pipeline, TextStreamer } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate pipeline\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA\",\n",
        "  { dtype: \"fp16\" },\n",
        ");\n",
        "\n",
        "// Create text streamer\n",
        "const streamer = new TextStreamer(generator.tokenizer, {\n",
        "  skip_prompt: true,\n",
        "  // callback_function: (text) => { }, // Optional callback function\n",
        "})\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  { role: \"user\", content: \"who is ai?\" },\n",
        "];\n",
        "\n",
        "// Generate a response\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false, streamer });\n",
        "console.log(output[0].generated_text.at(-1).content);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7E1mGw9M3-7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtUsKk8e42NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XyLJCNj42Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lwg72ddU42GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال ع المعالج"
      ],
      "metadata": {
        "id": "baFyrssI-NYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!node /content/3.js\n",
        "\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  dtype: 'fp32', // or 'fp16', 'int8', etc.\n",
        "  device: 'cpu', // or 'gpu' if WebGPU is available\n",
        "});\n",
        "\n",
        "// Run the model\n",
        "const out = await pipe('who is ai?');\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "metadata": {
        "id": "KBeQxyKa42CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/3.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDUXeF9c4kf7",
        "outputId": "3d9ab5c7-703c-4ee3-b8c2-5a7da01efd2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    generated_text: \u001b[32m'who is ai? what does it do?\\n'\u001b[39m +\n",
            "      \u001b[32m'A good AI (artificial intelligence) can be defined'\u001b[39m\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "aشغال"
      ],
      "metadata": {
        "id": "sKvx3exGAEXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/3.js\n",
        "\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  dtype: 'fp32', // or 'fp16', 'int8', etc.\n",
        "  device: 'cpu', // or 'gpu' if WebGPU is available\n",
        "});\n",
        "\n",
        "// Run the model\n",
        "const out = await pipe('who is ai?');\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "metadata": {
        "id": "v0muf_5VAB9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm i @huggingface/transformers\n"
      ],
      "metadata": {
        "id": "miaVhYWE4lsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  dtype: 'fp32', // or 'fp16', 'int8', etc.\n",
        "  device: 'cpu', // or 'gpu' if WebGPU is available\n",
        "});\n",
        "\n",
        "// Run the model\n",
        "const out = await pipe('who is ai?');\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6kNAJL2294HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFrxdHAF_2IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  device: 'cpu',\n",
        "});\n",
        "\n",
        "// Run the model with max_new_tokens and do_sample\n",
        "const out = await pipe('who is ai?', {\n",
        "  max_new_tokens: 50,  // Adjust as needed\n",
        "  do_sample: true,      // Set to false for deterministic generation\n",
        "});\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "n78xTqEl_2iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/4.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDNbqV3eAJH2",
        "outputId": "805bae29-ff0d-4e1a-b5a0-6eec2e534750"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype not specified for \"model\". Using the default dtype (fp32) for this device (cpu).\n",
            "[\n",
            "  {\n",
            "    generated_text: \u001b[32m'who is ai? 360\\n'\u001b[39m +\n",
            "      \u001b[32m'\\n'\u001b[39m +\n",
            "      \u001b[32m'Assistant: AI (Artificial Intelligence) refers to the field of computer science that focuses on the development, operation and application of intelligent programs or systems. It builds on principles such as machine learning, natural language processing, vision and'\u001b[39m\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "شغال معالج\n",
        "!node /content/4.js\n",
        "\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  device: 'cpu',\n",
        "});\n",
        "\n",
        "// Run the model with max_new_tokens and do_sample\n",
        "const out = await pipe('who is ai?', {\n",
        "  max_new_tokens: 50,  // Adjust as needed\n",
        "  do_sample: true,      // Set to false for deterministic generation\n",
        "});\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "metadata": {
        "id": "smIsXNgLAKl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx-community/YuE-s1-7B-anneal-en-cot-ONNX"
      ],
      "metadata": {
        "id": "jqlBZD50A_FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/YuE-s1-7B-anneal-en-cot-ONNX', {\n",
        "  device: 'cpu',\n",
        "});\n",
        "\n",
        "// Run the model with max_new_tokens and do_sample\n",
        "const out = await pipe('who is ai?', {\n",
        "  max_new_tokens: 50,  // Adjust as needed\n",
        "  do_sample: true,      // Set to false for deterministic generation\n",
        "});\n",
        "\n",
        "// Display the results\n",
        "console.log(out);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Tz22Is8WBBff",
        "outputId": "56ddf822-cff0-417a-cc76-c10a82f4a3e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-180a823f8061>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-180a823f8061>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import { pipeline } from '@huggingface/transformers';\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/4.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYWqkDuDBOTP",
        "outputId": "c554e797-54d3-4a43-8eaa-068941cfbb80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/4.js:5\n",
            "  { dtype: \"q4f16\" },\n",
            "  ^\n",
            "\n",
            "SyntaxError: Unexpected token '{'\n",
            "\u001b[90m    at ModuleLoader.moduleStrategy (node:internal/modules/esm/translators:152:18)\u001b[39m\n",
            "\u001b[90m    at ModuleLoader.moduleProvider (node:internal/modules/esm/loader:299:14)\u001b[39m\n",
            "\u001b[90m    at async link (node:internal/modules/esm/module_job:67:21)\u001b[39m\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\n",
        "  { dtype: \"q4f16\" },\n",
        ");"
      ],
      "metadata": {
        "id": "ySEHBkMjBZpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const pipe = await pipeline('text-generation', './YuE-s1-7B-anneal-en-cot-ONNX', { // load locally\n",
        "  device: 'cpu',\n",
        "  dtype: \"q4f16\"\n",
        "});\n",
        "\n",
        "const out = await pipe('who is ai?', {\n",
        "  max_new_tokens: 50,\n",
        "  do_sample: true,\n",
        "});\n",
        "\n",
        "console.log(out);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qNoyvGvMB6ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/4.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4g5UIv_B_Tv",
        "outputId": "13b8873e-64f0-475a-828f-cf0a1631ae38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@huggingface/transformers/dist/transformers.mjs:31095\n",
            "    throw Error(`${message}: \"${remoteURL}\".`);\n",
            "          ^\n",
            "\n",
            "Error: Unauthorized access to file: \"https://huggingface.co/./YuE-s1-7B-anneal-en-cot-ONNX/resolve/main/config.json\".\n",
            "    at handleError \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31095:11\u001b[90m)\u001b[39m\n",
            "    at getModelFile \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31328:24\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async getModelJSON \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31427:18\u001b[90m)\u001b[39m\n",
            "    at async loadConfig \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:5603:12\u001b[90m)\u001b[39m\n",
            "    at async PretrainedConfig.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:5924:32\u001b[90m)\u001b[39m\n",
            "    at async AutoModelForCausalLM.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:15417:26\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 1)\n",
            "    at async loadItems \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24602:5\u001b[90m)\u001b[39m\n",
            "    at async pipeline \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:24532:21\u001b[90m)\u001b[39m\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "os.environ['HF_READ_TOKEN'] = 'ء'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yViKny29CGHC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install @xenova/transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgono6zMB_o2",
        "outputId": "fc1aef2e-7bf1-4c75-bc60-bb3a10dacc3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "added 57 packages, and audited 140 packages in 24s\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K30 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@xenova/transformers';\n",
        "\n",
        "(async () => {\n",
        "    const pipe = await pipeline('text-generation', './YuE-s1-7B-anneal-en-cot-ONNX', { quantized: true });\n",
        "\n",
        "    const out = await pipe('Who is AI?', {\n",
        "        max_new_tokens: 50,\n",
        "        do_sample: true,\n",
        "    });\n",
        "\n",
        "    console.log(out);\n",
        "})();\n"
      ],
      "metadata": {
        "id": "m4f1zBuaCilV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/4.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dacvHylpCn0m",
        "outputId": "794c4d1e-ed5e-453e-f119-5cd9265cc817"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@xenova/transformers/src/utils/hub.js:238\n",
            "    throw Error(`${message}: \"${remoteURL}\".`);\n",
            "          ^\n",
            "\n",
            "Error: Unauthorized access to file: \"https://huggingface.co/YuE-s1-7B-anneal-en-cot-ONNX/resolve/main/tokenizer_config.json\".\n",
            "    at handleError \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/utils/hub.js:238:11\u001b[90m)\u001b[39m\n",
            "    at getModelFile \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/utils/hub.js:471:24\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async getModelJSON \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/utils/hub.js:572:18\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 1)\n",
            "    at async loadTokenizer \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/tokenizers.js:61:18\u001b[90m)\u001b[39m\n",
            "    at async AutoTokenizer.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/tokenizers.js:4459:50\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 0)\n",
            "    at async loadItems \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/pipelines.js:3279:5\u001b[90m)\u001b[39m\n",
            "    at async pipeline \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/pipelines.js:3219:21\u001b[90m)\u001b[39m\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "const pipe = await pipeline('text-generation', 'YuE-s1-7B-anneal-en-cot-ONNX', {\n",
        "    token: \"hf_xxxxx\"\n",
        "});\n"
      ],
      "metadata": {
        "id": "wgP8LYhbCwYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تأكد من تنزيل النموذج في Google Colab"
      ],
      "metadata": {
        "id": "qygfu1k3Da9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/YuE-s1-7B-anneal-en-cot-ONNX\n"
      ],
      "metadata": {
        "id": "sq62lQ-ZDWXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ثم تحقق من وجود الملفات داخل المجلد:"
      ],
      "metadata": {
        "id": "b0hds7ELDc23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la YuE-s1-7B-anneal-en-cot-ONNX\n"
      ],
      "metadata": {
        "id": "Ue4vXqj0DYcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@xenova/transformers';\n",
        "\n",
        "(async () => {\n",
        "    const modelPath = '/content/YuE-s1-7B-anneal-en-cot-ONNX';  // المسار بعد التنزيل\n",
        "\n",
        "    const pipe = await pipeline('text-generation', modelPath, {\n",
        "        local_files_only: true  // تأكيد تحميل النموذج من المسار المحلي فقط\n",
        "    });\n",
        "\n",
        "    const out = await pipe('Who is AI?', {\n",
        "        max_new_tokens: 50,\n",
        "        do_sample: true,\n",
        "    });\n",
        "\n",
        "    console.log(out);\n",
        "})();\n"
      ],
      "metadata": {
        "id": "NbEucCsyDflx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@xenova/transformers';\n",
        "\n",
        "(async () => {\n",
        "    const modelPath = '/content/gpt2_onnx';  // المسار بعد التنزيل\n",
        "\n",
        "    const pipe = await pipeline('text-generation', modelPath, {\n",
        "        local_files_only: true  // تأكيد تحميل النموذج من المسار المحلي فقط\n",
        "    });\n",
        "\n",
        "    const out = await pipe('Who is AI?', {\n",
        "        max_new_tokens: 50,\n",
        "        do_sample: true,\n",
        "    });\n",
        "\n",
        "    console.log(out);\n",
        "})();\n"
      ],
      "metadata": {
        "id": "v5lLSpp9Dh1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/4.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLaUlQxQDon6",
        "outputId": "50fb59b0-0392-4d0a-bd7e-5ddf445f9990"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@xenova/transformers/src/utils/hub.js:459\n",
            "                    throw Error(`\\`local_files_only=true\\` or \\`env.allowRemoteModels=false\\` and file was not found locally at \"${localPath}\".`);\n",
            "                          ^\n",
            "\n",
            "Error: `local_files_only=true` or `env.allowRemoteModels=false` and file was not found locally at \"/content/node_modules/@xenova/transformers/models/content/gpt2_onnx/model.onnx/tokenizer.json\".\n",
            "    at getModelFile \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/utils/hub.js:459:27\u001b[90m)\u001b[39m\n",
            "    at async getModelJSON \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/utils/hub.js:572:18\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 0)\n",
            "    at async loadTokenizer \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/tokenizers.js:61:18\u001b[90m)\u001b[39m\n",
            "    at async AutoTokenizer.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/tokenizers.js:4459:50\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 0)\n",
            "    at async loadItems \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/pipelines.js:3279:5\u001b[90m)\u001b[39m\n",
            "    at async pipeline \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@xenova\u001b[24m/transformers/src/pipelines.js:3219:21\u001b[90m)\u001b[39m\n",
            "    at async \u001b[90mfile:///content/\u001b[39m4.js:4:18\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@xenova/transformers';\n",
        "\n",
        "(async () => {\n",
        "    const pipe = await pipeline('text-generation', '/content/gpt2_onnx/model.onnx', {\n",
        "        local_files_only: true,\n",
        "        quantized: true\n",
        "    });\n",
        "\n",
        "    const out = await pipe('Who is AI?', {\n",
        "        max_new_tokens: 50,\n",
        "        do_sample: true,\n",
        "    });\n",
        "\n",
        "    console.log(out);\n",
        "})();\n"
      ],
      "metadata": {
        "id": "N1X-p6UbDo9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# تحميل النموذج\n",
        "session = ort.InferenceSession(\"/content/gpt2_onnx/model.onnx\")\n",
        "\n",
        "# إنشاء إدخال تجريبي\n",
        "input_data = np.random.rand(1, 128).astype(np.float32)  # غيّر بناءً على المدخل المطلوب\n",
        "\n",
        "# تنفيذ الاستدلال\n",
        "outputs = session.run(None, {\"input\": input_data})\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "6fZSrYtDEh5H",
        "outputId": "63c292c5-8a51-4756-97c9-452b918faa24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Required inputs (['input_ids', 'past_key_values.0.key', 'past_key_values.0.value', 'past_key_values.1.key', 'past_key_values.1.value', 'past_key_values.2.key', 'past_key_values.2.value', 'past_key_values.3.key', 'past_key_values.3.value', 'past_key_values.4.key', 'past_key_values.4.value', 'past_key_values.5.key', 'past_key_values.5.value', 'past_key_values.6.key', 'past_key_values.6.value', 'past_key_values.7.key', 'past_key_values.7.value', 'past_key_values.8.key', 'past_key_values.8.value', 'past_key_values.9.key', 'past_key_values.9.value', 'past_key_values.10.key', 'past_key_values.10.value', 'past_key_values.11.key', 'past_key_values.11.value', 'attention_mask', 'position_ids']) are missing from input feed (['input']).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ea2c6e13e6d4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# تنفيذ الاستدلال\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \"\"\"\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, feed_input_names)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mmissing_input_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_input_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;34mf\"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Required inputs (['input_ids', 'past_key_values.0.key', 'past_key_values.0.value', 'past_key_values.1.key', 'past_key_values.1.value', 'past_key_values.2.key', 'past_key_values.2.value', 'past_key_values.3.key', 'past_key_values.3.value', 'past_key_values.4.key', 'past_key_values.4.value', 'past_key_values.5.key', 'past_key_values.5.value', 'past_key_values.6.key', 'past_key_values.6.value', 'past_key_values.7.key', 'past_key_values.7.value', 'past_key_values.8.key', 'past_key_values.8.value', 'past_key_values.9.key', 'past_key_values.9.value', 'past_key_values.10.key', 'past_key_values.10.value', 'past_key_values.11.key', 'past_key_values.11.value', 'attention_mask', 'position_ids']) are missing from input feed (['input'])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ayhgشغال"
      ],
      "metadata": {
        "id": "oSF7ZmsOFPw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Allocate a pipeline for sentiment-analysis\n",
        "pipe = pipeline('sentiment-analysis')\n",
        "\n",
        "out = pipe('I love transformers!')\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "94d99225d4e74b358717dc60b7d95eac",
            "c8920e8f0b5d427cafba59ab3253e662",
            "04fd3b9500364967a02f29628cda18c4",
            "4a3c52153c194eed883fcc772321378e",
            "ff1de6d475914d76afd6cd0a174cad03",
            "25d88f228f7f4305b8535b5866b98325",
            "1db0b0449db44a18bab6dbe14ea32775",
            "968fe5dad89c499e8ed16b26fdc012ad",
            "837327f8ef2b4da6a2f63c2fa1c5581f",
            "1696f0004eff434c84412cc0894487e0",
            "a29840c26b434bfbb115838bed902ccb",
            "84fcebcdbe4241fdaaa9bb6b6b1d1482",
            "a015ae13575d40b696e3f4768d6e8917",
            "3be6c08f610048a3af7283ce5b8febd2",
            "954ad9192c704b4ea433d75e2c0946f8",
            "e2f48c331b654469998c1daefcdfb18d",
            "d39e124cde4040d18ad3055af9983a83",
            "4219c602ee1a42c2a6fc3b38f22be491",
            "69900db3b72a4191b93c013c5fc9658f",
            "2f7888fa538b4fcb96414df032816625",
            "3c0fa39d1e754867b1cb456896f22dd9",
            "911a80d97a7041978f20933d42b6654e",
            "b1e913b93d9f4ef1905aa5d2f1216574",
            "07456077e17b494c8f2524ab5056dd6c",
            "a6931a8fe57e4e50ae0dcfd41907530c",
            "463caf3c561b4c7cbce3ebb6d2668d99",
            "be32c8153c0245c39ea6b66da1ae11cb",
            "4d8c398e20e04fdb8ff91ada95601010",
            "7e0a8562c6a4455c8cf991fde4641a43",
            "87376f17e4e74d38a9c59eb5845b2919",
            "e6e957d215c1419f83ef20b4e8bbff1c",
            "ce7fdde86a8a4700a7506edfe0f44747",
            "668f3f9d5c934a7ab5aaabeac8f942b4",
            "d05401d92ffa4292b6ff6f001e493d73",
            "991466e438a7497c9691e862a55857ad",
            "492a47f12a394d008c7d30c61f185eab",
            "3d5c9a1970ab4fee8cfca2ccce86bd4f",
            "ec2aac32a4764525bc66cc38ec4f6a88",
            "46cedc2a32864ec78f62403f5a48818a",
            "826d7d33d2de437082b90086f2651f02",
            "faf23ace225745a59b40e3b099c99c4c",
            "c5142fe227bb4a20ace3cd6ecec77201",
            "564991ab89414ca992d568ec49261878",
            "742dd84049e44286a5f44807769ba8b4"
          ]
        },
        "id": "A84OPhxxE_9o",
        "outputId": "dc6f46e8-1898-4485-dbf8-49cad4b7b720"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94d99225d4e74b358717dc60b7d95eac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84fcebcdbe4241fdaaa9bb6b6b1d1482"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1e913b93d9f4ef1905aa5d2f1216574"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d05401d92ffa4292b6ff6f001e493d73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UurAG9HtFAf-",
        "outputId": "45a8d8c1-95c5-45ee-afbb-2bc9522b30e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998069405555725}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate a pipeline for sentiment-analysis\n",
        "const pipe = await pipeline('sentiment-analysis');\n",
        "\n",
        "const out = await pipe('I love transformers!');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]"
      ],
      "metadata": {
        "id": "Lz0a54TIFsQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers.js/index"
      ],
      "metadata": {
        "id": "eipNy35SFiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "التكميم.dtype\"fp32\"\"fp16\"\"q8\"\"q4\""
      ],
      "metadata": {
        "id": "jhKzIB_RFlVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Use a different model for sentiment-analysis\n",
        "const pipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');"
      ],
      "metadata": {
        "id": "L5itYJEbFNSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// Run the model on WebGPU\n",
        "const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', {\n",
        "  device: 'webgpu',\n",
        "});"
      ],
      "metadata": {
        "id": "c7QYX8HpFd12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// Run the model at 4-bit quantization\n",
        "const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', {\n",
        "  dtype: 'q4',\n",
        "});"
      ],
      "metadata": {
        "id": "Ipv3SJARFf5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Run the model at 4-bit quantization\n",
        "const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', {\n",
        "  dtype: 'fp32',\n",
        "});\n",
        "\n",
        "const out = await pipe('I love transformers!');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]"
      ],
      "metadata": {
        "id": "ra2fDfAAFwUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/5.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXy9dW1zGBGI",
        "outputId": "fab07b07-46a8-4749-f873-5916129e3fa8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ { label: \u001b[32m'POSITIVE'\u001b[39m, score: \u001b[33m0.9998068809509277\u001b[39m } ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "drtwSsTVGXSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "شغال\n",
        "!node /content/5.js\n",
        "\n",
        "\n",
        "\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Run the model at 4-bit quantization\n",
        "const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', {\n",
        "  dtype: 'fp32',\n",
        "});\n",
        "\n",
        "const out = await pipe('I love transformers!');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]\n",
        "console.log(out);\n"
      ],
      "metadata": {
        "id": "Fwky6PSyGDyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Run the model at 4-bit quantization\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  dtype: 'fp32',\n",
        "});\n",
        "\n",
        "const out = await pipe('who is python?');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]\n",
        "console.log(out);"
      ],
      "metadata": {
        "id": "-TjzpDOuGe6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/5.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktSHNHUfG8Jr",
        "outputId": "40f75513-67c8-456f-8cfd-0b8328a1d10d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    generated_text: \u001b[32m'who is python? how should i use it?\\n'\u001b[39m +\n",
            "      \u001b[32m'\\n'\u001b[39m +\n",
            "      \u001b[32m'Python is a versatile and popular programming language used for'\u001b[39m\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "zKBCi7N_HLDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "شغال\n",
        "!node /content/5.js\n",
        "\n",
        "\n",
        "\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Run the model at 4-bit quantization\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA', {\n",
        "  dtype: 'fp32',\n",
        "});\n",
        "\n",
        "const out = await pipe('who is python?');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]\n",
        "console.log(out);\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UQV153hNG8gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GgZ2MUXHOmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEBjV-nzHOjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "لايعمل"
      ],
      "metadata": {
        "id": "NlMlY_fcHqUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Run the model at 4-bit quantization\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/YuE-s1-7B-anneal-en-cot-ONNX', {\n",
        "  dtype: 'fp32',\n",
        "});\n",
        "\n",
        "const out = await pipe('who is python?');\n",
        "// [{'label': 'POSITIVE', 'score': 0.999817686}]\n",
        "console.log(out);"
      ],
      "metadata": {
        "id": "ogNd0CgzHOd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/5.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbsl_nMFHaWR",
        "outputId": "560426b4-6346-44c1-ebb1-d6cc1f780168"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@huggingface/transformers/dist/transformers.mjs:31095\n",
            "    throw Error(`${message}: \"${remoteURL}\".`);\n",
            "          ^\n",
            "\n",
            "Error: Could not locate file: \"https://huggingface.co/onnx-community/YuE-s1-7B-anneal-en-cot-ONNX/resolve/main/onnx/model.onnx\".\n",
            "    at handleError \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31095:11\u001b[90m)\u001b[39m\n",
            "    at getModelFile \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31328:24\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async getSession \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:8703:20\u001b[90m)\u001b[39m\n",
            "    at async \u001b[90mfile:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:8720:65\n",
            "    at async Promise.all (index 0)\n",
            "    at async constructSessions \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:8718:31\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 0)\n",
            "    at async LlamaForCausalLM.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:9411:20\u001b[90m)\u001b[39m\n",
            "    at async AutoModelForCausalLM.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:15428:20\u001b[90m)\u001b[39m\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX"
      ],
      "metadata": {
        "id": "s31HoDLqHarP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/5.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ngwFOMbHv87",
        "outputId": "8a2506fd-b524-46fd-87f4-e7e0d8e6a786"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:///content/node_modules/@huggingface/transformers/dist/transformers.mjs:31095\n",
            "    throw Error(`${message}: \"${remoteURL}\".`);\n",
            "          ^\n",
            "\n",
            "Error: Could not locate file: \"https://huggingface.co/Xenova/Qwen1.5-1.8B-Chat/resolve/main/onnx/model.onnx\".\n",
            "    at handleError \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31095:11\u001b[90m)\u001b[39m\n",
            "    at getModelFile \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:31328:24\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async getSession \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:8703:20\u001b[90m)\u001b[39m\n",
            "    at async \u001b[90mfile:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:8720:65\n",
            "    at async Promise.all (index 0)\n",
            "    at async constructSessions \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:8718:31\u001b[90m)\u001b[39m\n",
            "    at async Promise.all (index 0)\n",
            "    at async Qwen2ForCausalLM.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:9411:20\u001b[90m)\u001b[39m\n",
            "    at async AutoModelForCausalLM.from_pretrained \u001b[90m(file:///content/\u001b[39mnode_modules/\u001b[4m@huggingface\u001b[24m/transformers/dist/transformers.mjs:15428:20\u001b[90m)\u001b[39m\n",
            "\n",
            "Node.js v18.20.5\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install onnxruntime"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDFLd40-JmAG",
        "outputId": "91e87f5c-114f-43b7-c3ba-31b47a85c9d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!npm install onnxruntime-node"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6ESqNxvJmQ1",
        "outputId": "166aef81-a220-4e33-caf8-240c5820d2e0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
            "up to date, audited 140 packages in 2s\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K30 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "  // Allocate pipeline\n",
        "  const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA');"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Cp_a-1zoJm3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate pipeline\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA');\n",
        "\n",
        "// Define the prompt for text generation\n",
        "const prompt = \"Write a short story about a robot who learns to love:\";\n",
        "\n",
        "// Generate text based on the prompt\n",
        "const generatedText = await pipe(prompt, {\n",
        "  max_new_tokens: 100, // Adjust the number of tokens to generate\n",
        "});\n",
        "\n",
        "// Print the generated text to the console\n",
        "console.log(generatedText[0].generated_text);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BrTUltaFJx_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Should be one of: auto, fp32, fp16, q8, int8, uint8, q4, bnb4, q4f16"
      ],
      "metadata": {
        "id": "DGOtpc8fIqCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/6.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFUnLlyRHwNY",
        "outputId": "50207774-94d4-4c3e-d519-560f866a9363"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype not specified for \"model\". Using the default dtype (fp32) for this device (cpu).\n",
            "Write a short story about a robot who learns to love: A young adult robot named Mira goes on a trip around the world, meeting new families and people. She discovers that she hasn't been able to express her feelings with her human friends due to being stuck inside a computer program for so long. But when she sees that people from all over the world need someone like her - someone who can help them cope with their homesickness - Mira decides to take action and build a new robot. Along the way, she meets people from all around the globe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال ع المعالج كولاب"
      ],
      "metadata": {
        "id": "oE-gdmIEKboj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "شغال\n",
        "!node /content/6.js\n",
        "\n",
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "// Allocate pipeline\n",
        "const pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct-ONNX-MHA');\n",
        "\n",
        "// Define the prompt for text generation\n",
        "const prompt = \"Write a short story about a robot who learns to love:\";\n",
        "\n",
        "// Generate text based on the prompt\n",
        "const generatedText = await pipe(prompt, {\n",
        "  max_new_tokens: 100, // Adjust the number of tokens to generate\n",
        "});\n",
        "\n",
        "// Print the generated text to the console\n",
        "console.log(generatedText[0].generated_text);"
      ],
      "metadata": {
        "id": "OVgioPLWKXVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRCjmbBEKXSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1altCJ6KXPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_AkdaxvuKXLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mfG3HgcqJ8CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline, TextStreamer } from \"@huggingface/transformers\";\n",
        "\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\n",
        "  { dtype: \"q4f16\" },\n",
        ");\n",
        "\n",
        "const messages = [\n",
        "  { role: \"user\", content:  \"Solve the equation: x^2 - 3x + 2 = 0\" },\n",
        "];\n",
        "\n",
        "const streamer = new TextStreamer(generator.tokenizer, {\n",
        "  skip_prompt: true,\n",
        "});\n",
        "\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false, streamer });\n",
        "console.log(output[0].generated_text.at(-1).content);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FdexhSGKKNUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import { pipeline } from '@huggingface/transformers';\n",
        "\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"/content/gpt2_onnx\", // Path to your locally downloaded model\n",
        ");\n",
        "\n",
        "const prompt = \"Write a story about a cat who goes on an adventure:\";\n",
        "\n",
        "const generatedText = await generator(prompt, {\n",
        "  max_new_tokens: 100,\n",
        "});\n",
        "\n",
        "console.log(generatedText[0].generated_text);"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Mx9AFBojKN4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIg9VkE5KqBy",
        "outputId": "29bd0e4e-f1ae-4ce5-91fe-797aaaf97e6d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                            Version\n",
            "---------------------------------- ------------------\n",
            "absl-py                            1.4.0\n",
            "accelerate                         1.3.0\n",
            "aiohappyeyeballs                   2.4.6\n",
            "aiohttp                            3.11.12\n",
            "aiosignal                          1.3.2\n",
            "alabaster                          1.0.0\n",
            "albucore                           0.0.23\n",
            "albumentations                     2.0.3\n",
            "ale-py                             0.10.1\n",
            "altair                             5.5.0\n",
            "annotated-types                    0.7.0\n",
            "anyio                              3.7.1\n",
            "argon2-cffi                        23.1.0\n",
            "argon2-cffi-bindings               21.2.0\n",
            "array_record                       0.6.0\n",
            "arviz                              0.20.0\n",
            "astropy                            7.0.0\n",
            "astropy-iers-data                  0.2025.2.3.0.32.42\n",
            "astunparse                         1.6.3\n",
            "atpublic                           4.1.0\n",
            "attrs                              25.1.0\n",
            "audioread                          3.0.1\n",
            "autograd                           1.7.0\n",
            "babel                              2.17.0\n",
            "backcall                           0.2.0\n",
            "beautifulsoup4                     4.13.3\n",
            "bigframes                          1.35.0\n",
            "bigquery-magics                    0.5.0\n",
            "bleach                             6.2.0\n",
            "blinker                            1.9.0\n",
            "blis                               0.7.11\n",
            "blosc2                             3.0.0\n",
            "bokeh                              3.6.3\n",
            "Bottleneck                         1.4.2\n",
            "bqplot                             0.12.44\n",
            "branca                             0.8.1\n",
            "CacheControl                       0.14.2\n",
            "cachetools                         5.5.1\n",
            "catalogue                          2.0.10\n",
            "certifi                            2025.1.31\n",
            "cffi                               1.17.1\n",
            "chardet                            5.2.0\n",
            "charset-normalizer                 3.4.1\n",
            "chex                               0.1.88\n",
            "clarabel                           0.10.0\n",
            "click                              8.1.8\n",
            "cloudpathlib                       0.20.0\n",
            "cloudpickle                        3.1.1\n",
            "cmake                              3.31.4\n",
            "cmdstanpy                          1.2.5\n",
            "colorcet                           3.1.0\n",
            "coloredlogs                        15.0.1\n",
            "colorlover                         0.3.0\n",
            "colour                             0.1.5\n",
            "community                          1.0.0b1\n",
            "confection                         0.1.5\n",
            "cons                               0.4.6\n",
            "contourpy                          1.3.1\n",
            "cramjam                            2.9.1\n",
            "cryptography                       43.0.3\n",
            "cuda-python                        12.6.0\n",
            "cudf-cu12                          24.12.0\n",
            "cufflinks                          0.17.3\n",
            "cupy-cuda12x                       13.3.0\n",
            "cvxopt                             1.3.2\n",
            "cvxpy                              1.6.0\n",
            "cycler                             0.12.1\n",
            "cyipopt                            1.5.0\n",
            "cymem                              2.0.11\n",
            "Cython                             3.0.11\n",
            "dask                               2024.10.0\n",
            "datascience                        0.17.6\n",
            "datasets                           3.2.0\n",
            "db-dtypes                          1.4.1\n",
            "dbus-python                        1.2.18\n",
            "debugpy                            1.8.0\n",
            "decorator                          4.4.2\n",
            "defusedxml                         0.7.1\n",
            "Deprecated                         1.2.18\n",
            "diffusers                          0.32.2\n",
            "dill                               0.3.8\n",
            "distro                             1.9.0\n",
            "dlib                               19.24.2\n",
            "dm-tree                            0.1.9\n",
            "docker-pycreds                     0.4.0\n",
            "docstring_parser                   0.16\n",
            "docutils                           0.21.2\n",
            "dopamine_rl                        4.1.2\n",
            "duckdb                             1.1.3\n",
            "earthengine-api                    1.5.1\n",
            "easydict                           1.13\n",
            "editdistance                       0.8.1\n",
            "eerepr                             0.1.0\n",
            "einops                             0.8.0\n",
            "en-core-web-sm                     3.7.1\n",
            "entrypoints                        0.4\n",
            "et_xmlfile                         2.0.0\n",
            "etils                              1.11.0\n",
            "etuples                            0.3.9\n",
            "evaluate                           0.4.3\n",
            "Farama-Notifications               0.0.4\n",
            "fastai                             2.7.18\n",
            "fastcore                           1.7.29\n",
            "fastdownload                       0.0.7\n",
            "fastjsonschema                     2.21.1\n",
            "fastprogress                       1.0.3\n",
            "fastrlock                          0.8.3\n",
            "filelock                           3.17.0\n",
            "firebase-admin                     6.6.0\n",
            "Flask                              3.1.0\n",
            "flatbuffers                        25.2.10\n",
            "flax                               0.10.2\n",
            "folium                             0.19.4\n",
            "fonttools                          4.55.8\n",
            "frozendict                         2.4.6\n",
            "frozenlist                         1.5.0\n",
            "fsspec                             2024.9.0\n",
            "future                             1.0.0\n",
            "gast                               0.6.0\n",
            "gcsfs                              2024.10.0\n",
            "GDAL                               3.6.4\n",
            "gdown                              5.2.0\n",
            "geemap                             0.35.1\n",
            "gensim                             4.3.3\n",
            "geocoder                           1.38.1\n",
            "geographiclib                      2.0\n",
            "geopandas                          1.0.1\n",
            "geopy                              2.4.1\n",
            "gin-config                         0.5.0\n",
            "gitdb                              4.0.12\n",
            "GitPython                          3.1.44\n",
            "glob2                              0.7\n",
            "google                             2.0.3\n",
            "google-ai-generativelanguage       0.6.15\n",
            "google-api-core                    2.19.2\n",
            "google-api-python-client           2.160.0\n",
            "google-auth                        2.27.0\n",
            "google-auth-httplib2               0.2.0\n",
            "google-auth-oauthlib               1.2.1\n",
            "google-cloud-aiplatform            1.79.0\n",
            "google-cloud-bigquery              3.25.0\n",
            "google-cloud-bigquery-connection   1.17.0\n",
            "google-cloud-bigquery-storage      2.28.0\n",
            "google-cloud-bigtable              2.28.1\n",
            "google-cloud-core                  2.4.1\n",
            "google-cloud-dataproc              5.16.0\n",
            "google-cloud-datastore             2.20.2\n",
            "google-cloud-firestore             2.20.0\n",
            "google-cloud-functions             1.19.0\n",
            "google-cloud-iam                   2.17.0\n",
            "google-cloud-language              2.16.0\n",
            "google-cloud-pubsub                2.25.0\n",
            "google-cloud-resource-manager      1.14.0\n",
            "google-cloud-spanner               3.51.0\n",
            "google-cloud-storage               2.19.0\n",
            "google-cloud-translate             3.19.0\n",
            "google-colab                       1.0.0\n",
            "google-crc32c                      1.6.0\n",
            "google-genai                       0.8.0\n",
            "google-generativeai                0.8.4\n",
            "google-pasta                       0.2.0\n",
            "google-resumable-media             2.7.2\n",
            "google-spark-connect               0.5.1\n",
            "googleapis-common-protos           1.66.0\n",
            "googledrivedownloader              1.1.0\n",
            "graphviz                           0.20.3\n",
            "greenlet                           3.1.1\n",
            "grpc-google-iam-v1                 0.14.0\n",
            "grpc-interceptor                   0.15.4\n",
            "grpcio                             1.70.0\n",
            "grpcio-status                      1.62.3\n",
            "gspread                            6.1.4\n",
            "gspread-dataframe                  4.0.0\n",
            "gym                                0.25.2\n",
            "gym-notices                        0.0.8\n",
            "gymnasium                          1.0.0\n",
            "h11                                0.14.0\n",
            "h5netcdf                           1.5.0\n",
            "h5py                               3.12.1\n",
            "highspy                            1.9.0\n",
            "holidays                           0.66\n",
            "holoviews                          1.20.0\n",
            "html5lib                           1.1\n",
            "httpcore                           1.0.7\n",
            "httpimport                         1.4.0\n",
            "httplib2                           0.22.0\n",
            "httpx                              0.28.1\n",
            "huggingface-hub                    0.28.1\n",
            "humanfriendly                      10.0\n",
            "humanize                           4.11.0\n",
            "hyperopt                           0.2.7\n",
            "ibis-framework                     9.2.0\n",
            "idna                               3.10\n",
            "imageio                            2.37.0\n",
            "imageio-ffmpeg                     0.6.0\n",
            "imagesize                          1.4.1\n",
            "imbalanced-learn                   0.13.0\n",
            "imgaug                             0.4.0\n",
            "immutabledict                      4.2.1\n",
            "importlib_metadata                 8.6.1\n",
            "importlib_resources                6.5.2\n",
            "imutils                            0.5.4\n",
            "inflect                            7.5.0\n",
            "iniconfig                          2.0.0\n",
            "intel-cmplr-lib-ur                 2025.0.4\n",
            "intel-openmp                       2025.0.4\n",
            "ipyevents                          2.0.2\n",
            "ipyfilechooser                     0.6.0\n",
            "ipykernel                          5.5.6\n",
            "ipyleaflet                         0.19.2\n",
            "ipyparallel                        8.8.0\n",
            "ipython                            7.34.0\n",
            "ipython-genutils                   0.2.0\n",
            "ipython-sql                        0.5.0\n",
            "ipytree                            0.2.2\n",
            "ipywidgets                         7.7.1\n",
            "itsdangerous                       2.2.0\n",
            "jax                                0.4.33\n",
            "jax-cuda12-pjrt                    0.4.33\n",
            "jax-cuda12-plugin                  0.4.33\n",
            "jaxlib                             0.4.33\n",
            "jeepney                            0.7.1\n",
            "jellyfish                          1.1.0\n",
            "jieba                              0.42.1\n",
            "Jinja2                             3.1.5\n",
            "jiter                              0.8.2\n",
            "joblib                             1.4.2\n",
            "jsonpatch                          1.33\n",
            "jsonpickle                         4.0.1\n",
            "jsonpointer                        3.0.0\n",
            "jsonschema                         4.23.0\n",
            "jsonschema-specifications          2024.10.1\n",
            "jupyter-client                     6.1.12\n",
            "jupyter-console                    6.1.0\n",
            "jupyter_core                       5.7.2\n",
            "jupyter-leaflet                    0.19.2\n",
            "jupyter-server                     1.24.0\n",
            "jupyterlab_pygments                0.3.0\n",
            "jupyterlab_widgets                 3.0.13\n",
            "kaggle                             1.6.17\n",
            "kagglehub                          0.3.6\n",
            "keras                              3.8.0\n",
            "keras-hub                          0.18.1\n",
            "keras-nlp                          0.18.1\n",
            "keyring                            23.5.0\n",
            "kiwisolver                         1.4.8\n",
            "langchain                          0.3.17\n",
            "langchain-core                     0.3.33\n",
            "langchain-text-splitters           0.3.5\n",
            "langcodes                          3.5.0\n",
            "langsmith                          0.3.6\n",
            "language_data                      1.3.0\n",
            "launchpadlib                       1.10.16\n",
            "lazr.restfulclient                 0.14.4\n",
            "lazr.uri                           1.0.6\n",
            "lazy_loader                        0.4\n",
            "libclang                           18.1.1\n",
            "libcudf-cu12                       24.12.0\n",
            "libkvikio-cu12                     24.12.1\n",
            "librosa                            0.10.2.post1\n",
            "lightgbm                           4.5.0\n",
            "linkify-it-py                      2.0.3\n",
            "llvmlite                           0.44.0\n",
            "locket                             1.0.0\n",
            "logical-unification                0.4.6\n",
            "lxml                               5.3.0\n",
            "marisa-trie                        1.2.1\n",
            "Markdown                           3.7\n",
            "markdown-it-py                     3.0.0\n",
            "MarkupSafe                         3.0.2\n",
            "matplotlib                         3.10.0\n",
            "matplotlib-inline                  0.1.7\n",
            "matplotlib-venn                    1.1.1\n",
            "mdit-py-plugins                    0.4.2\n",
            "mdurl                              0.1.2\n",
            "miniKanren                         1.0.3\n",
            "missingno                          0.5.2\n",
            "mistune                            3.1.1\n",
            "mizani                             0.13.1\n",
            "mkl                                2025.0.1\n",
            "ml-dtypes                          0.4.1\n",
            "mlxtend                            0.23.4\n",
            "more-itertools                     10.6.0\n",
            "moviepy                            1.0.3\n",
            "mpmath                             1.3.0\n",
            "msgpack                            1.1.0\n",
            "multidict                          6.1.0\n",
            "multipledispatch                   1.0.0\n",
            "multiprocess                       0.70.16\n",
            "multitasking                       0.0.11\n",
            "murmurhash                         1.0.12\n",
            "music21                            9.3.0\n",
            "namex                              0.0.8\n",
            "narwhals                           1.25.1\n",
            "natsort                            8.4.0\n",
            "nbclassic                          1.2.0\n",
            "nbclient                           0.10.2\n",
            "nbconvert                          7.16.6\n",
            "nbformat                           5.10.4\n",
            "ndindex                            1.9.2\n",
            "nest-asyncio                       1.6.0\n",
            "networkx                           3.4.2\n",
            "nibabel                            5.3.2\n",
            "nltk                               3.9.1\n",
            "notebook                           6.5.5\n",
            "notebook_shim                      0.2.4\n",
            "numba                              0.61.0\n",
            "numba-cuda                         0.0.17.1\n",
            "numexpr                            2.10.2\n",
            "numpy                              2.2.2\n",
            "nvidia-cublas-cu12                 12.4.5.8\n",
            "nvidia-cuda-cupti-cu12             12.4.127\n",
            "nvidia-cuda-nvcc-cu12              12.5.82\n",
            "nvidia-cuda-nvrtc-cu12             12.4.127\n",
            "nvidia-cuda-runtime-cu12           12.4.127\n",
            "nvidia-cudnn-cu12                  9.1.0.70\n",
            "nvidia-cufft-cu12                  11.2.1.3\n",
            "nvidia-curand-cu12                 10.3.5.147\n",
            "nvidia-cusolver-cu12               11.6.1.9\n",
            "nvidia-cusparse-cu12               12.3.1.170\n",
            "nvidia-cusparselt-cu12             0.6.2\n",
            "nvidia-nccl-cu12                   2.21.5\n",
            "nvidia-nvcomp-cu12                 4.1.0.6\n",
            "nvidia-nvjitlink-cu12              12.4.127\n",
            "nvidia-nvtx-cu12                   12.4.127\n",
            "nvtx                               0.2.10\n",
            "nx-cugraph-cu12                    24.12.0\n",
            "oauth2client                       4.1.3\n",
            "oauthlib                           3.2.2\n",
            "onnx                               1.17.0\n",
            "onnxruntime                        1.20.1\n",
            "onnxruntime-genai                  0.5.2\n",
            "openai                             1.61.1\n",
            "opencv-contrib-python              4.11.0.86\n",
            "opencv-python                      4.11.0.86\n",
            "opencv-python-headless             4.11.0.86\n",
            "openpyxl                           3.1.5\n",
            "opentelemetry-api                  1.16.0\n",
            "opentelemetry-sdk                  1.16.0\n",
            "opentelemetry-semantic-conventions 0.37b0\n",
            "opt_einsum                         3.4.0\n",
            "optax                              0.2.4\n",
            "optimum                            1.24.0\n",
            "optree                             0.14.0\n",
            "orbax-checkpoint                   0.6.4\n",
            "orjson                             3.10.15\n",
            "osqp                               0.6.7.post3\n",
            "packaging                          24.2\n",
            "pandas                             2.2.3\n",
            "pandas-datareader                  0.10.0\n",
            "pandas-gbq                         0.26.1\n",
            "pandas-stubs                       2.2.2.240909\n",
            "pandocfilters                      1.5.1\n",
            "panel                              1.6.0\n",
            "param                              2.2.0\n",
            "parso                              0.8.4\n",
            "parsy                              2.1\n",
            "partd                              1.4.2\n",
            "pathlib                            1.0.1\n",
            "patsy                              1.0.1\n",
            "peewee                             3.17.9\n",
            "peft                               0.14.0\n",
            "pexpect                            4.9.0\n",
            "pickleshare                        0.7.5\n",
            "pillow                             11.1.0\n",
            "pip                                24.1.2\n",
            "platformdirs                       4.3.6\n",
            "plotly                             5.24.1\n",
            "plotnine                           0.14.5\n",
            "pluggy                             1.5.0\n",
            "ply                                3.11\n",
            "polars                             1.9.0\n",
            "pooch                              1.8.2\n",
            "portpicker                         1.5.2\n",
            "preshed                            3.0.9\n",
            "prettytable                        3.14.0\n",
            "proglog                            0.1.10\n",
            "progressbar2                       4.5.0\n",
            "prometheus_client                  0.21.1\n",
            "promise                            2.3\n",
            "prompt_toolkit                     3.0.50\n",
            "propcache                          0.2.1\n",
            "prophet                            1.1.6\n",
            "proto-plus                         1.26.0\n",
            "protobuf                           5.29.3\n",
            "psutil                             5.9.5\n",
            "psycopg2                           2.9.10\n",
            "ptyprocess                         0.7.0\n",
            "py-cpuinfo                         9.0.0\n",
            "py4j                               0.10.9.7\n",
            "pyarrow                            19.0.0\n",
            "pyasn1                             0.6.1\n",
            "pyasn1_modules                     0.4.1\n",
            "pycocotools                        2.0.8\n",
            "pycparser                          2.22\n",
            "pydantic                           2.10.6\n",
            "pydantic_core                      2.27.2\n",
            "pydata-google-auth                 1.9.1\n",
            "pydot                              3.0.4\n",
            "pydotplus                          2.0.2\n",
            "PyDrive                            1.3.1\n",
            "PyDrive2                           1.21.3\n",
            "pyerfa                             2.0.1.5\n",
            "pygame                             2.6.1\n",
            "pygit2                             1.17.0\n",
            "Pygments                           2.18.0\n",
            "PyGObject                          3.42.1\n",
            "PyJWT                              2.10.1\n",
            "pylibcudf-cu12                     24.12.0\n",
            "pylibcugraph-cu12                  24.12.0\n",
            "pylibraft-cu12                     24.12.0\n",
            "pymc                               5.20.0\n",
            "pymystem3                          0.2.0\n",
            "pynvjitlink-cu12                   0.5.0\n",
            "pyogrio                            0.10.0\n",
            "Pyomo                              6.8.2\n",
            "PyOpenGL                           3.1.9\n",
            "pyOpenSSL                          24.2.1\n",
            "pyparsing                          3.2.1\n",
            "pyperclip                          1.9.0\n",
            "pyproj                             3.7.0\n",
            "pyshp                              2.3.1\n",
            "PySocks                            1.7.1\n",
            "pyspark                            3.5.4\n",
            "pytensor                           2.26.4\n",
            "pytest                             8.3.4\n",
            "python-apt                         0.0.0\n",
            "python-box                         7.3.2\n",
            "python-dateutil                    2.9.0.post0\n",
            "python-louvain                     0.16\n",
            "python-slugify                     8.0.4\n",
            "python-snappy                      0.7.3\n",
            "python-utils                       3.9.1\n",
            "pytz                               2025.1\n",
            "pyviz_comms                        3.0.4\n",
            "PyYAML                             6.0.2\n",
            "pyzmq                              24.0.1\n",
            "qdldl                              0.1.7.post5\n",
            "ratelim                            0.1.6\n",
            "referencing                        0.36.2\n",
            "regex                              2024.11.6\n",
            "requests                           2.32.3\n",
            "requests-oauthlib                  2.0.0\n",
            "requests-toolbelt                  1.0.0\n",
            "requirements-parser                0.9.0\n",
            "rich                               13.9.4\n",
            "rmm-cu12                           24.12.1\n",
            "rpds-py                            0.22.3\n",
            "rpy2                               3.4.2\n",
            "rsa                                4.9\n",
            "safetensors                        0.5.2\n",
            "scikit-image                       0.25.1\n",
            "scikit-learn                       1.6.1\n",
            "scipy                              1.13.1\n",
            "scooby                             0.10.0\n",
            "scs                                3.2.7.post2\n",
            "seaborn                            0.13.2\n",
            "SecretStorage                      3.3.1\n",
            "Send2Trash                         1.8.3\n",
            "sentence-transformers              3.4.1\n",
            "sentencepiece                      0.2.0\n",
            "sentry-sdk                         2.20.0\n",
            "setproctitle                       1.3.4\n",
            "setuptools                         75.1.0\n",
            "shap                               0.46.0\n",
            "shapely                            2.0.7\n",
            "shellingham                        1.5.4\n",
            "simple-parsing                     0.1.7\n",
            "simsimd                            6.2.1\n",
            "six                                1.17.0\n",
            "sklearn-compat                     0.1.3\n",
            "sklearn-pandas                     2.2.0\n",
            "slicer                             0.0.8\n",
            "smart-open                         7.1.0\n",
            "smmap                              5.0.2\n",
            "sniffio                            1.3.1\n",
            "snowballstemmer                    2.2.0\n",
            "soundfile                          0.13.1\n",
            "soupsieve                          2.6\n",
            "soxr                               0.5.0.post1\n",
            "spacy                              3.7.5\n",
            "spacy-legacy                       3.0.12\n",
            "spacy-loggers                      1.0.5\n",
            "spanner-graph-notebook             1.0.9\n",
            "Sphinx                             8.1.3\n",
            "sphinxcontrib-applehelp            2.0.0\n",
            "sphinxcontrib-devhelp              2.0.0\n",
            "sphinxcontrib-htmlhelp             2.1.0\n",
            "sphinxcontrib-jsmath               1.0.1\n",
            "sphinxcontrib-qthelp               2.0.0\n",
            "sphinxcontrib-serializinghtml      2.0.0\n",
            "SQLAlchemy                         2.0.37\n",
            "sqlglot                            25.6.1\n",
            "sqlparse                           0.5.3\n",
            "srsly                              2.5.1\n",
            "stanio                             0.5.1\n",
            "statsmodels                        0.14.4\n",
            "stringzilla                        3.11.3\n",
            "sympy                              1.13.1\n",
            "tables                             3.10.2\n",
            "tabulate                           0.9.0\n",
            "tbb                                2022.0.0\n",
            "tcmlib                             1.2.0\n",
            "tenacity                           9.0.0\n",
            "tensorboard                        2.18.0\n",
            "tensorboard-data-server            0.7.2\n",
            "tensorflow                         2.18.0\n",
            "tensorflow-datasets                4.9.7\n",
            "tensorflow-hub                     0.16.1\n",
            "tensorflow-io-gcs-filesystem       0.37.1\n",
            "tensorflow-metadata                1.16.1\n",
            "tensorflow-probability             0.25.0\n",
            "tensorflow-text                    2.18.1\n",
            "tensorstore                        0.1.71\n",
            "termcolor                          2.5.0\n",
            "terminado                          0.18.1\n",
            "text-unidecode                     1.3\n",
            "textblob                           0.19.0\n",
            "tf_keras                           2.18.0\n",
            "tf-slim                            1.1.0\n",
            "thinc                              8.2.5\n",
            "threadpoolctl                      3.5.0\n",
            "tifffile                           2025.1.10\n",
            "timm                               1.0.14\n",
            "tinycss2                           1.4.0\n",
            "tokenizers                         0.21.0\n",
            "toml                               0.10.2\n",
            "toolz                              0.12.1\n",
            "torch                              2.6.0\n",
            "torchaudio                         2.5.1+cu124\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.21.0\n",
            "tornado                            6.4.2\n",
            "tqdm                               4.67.1\n",
            "traitlets                          5.7.1\n",
            "traittypes                         0.2.1\n",
            "transformers                       4.48.3\n",
            "triton                             3.2.0\n",
            "tweepy                             4.15.0\n",
            "typeguard                          4.4.1\n",
            "typer                              0.15.1\n",
            "types-pytz                         2025.1.0.20250204\n",
            "types-setuptools                   75.8.0.20250210\n",
            "typing_extensions                  4.12.2\n",
            "tzdata                             2025.1\n",
            "tzlocal                            5.2\n",
            "uc-micro-py                        1.0.3\n",
            "umf                                0.9.1\n",
            "uritemplate                        4.1.1\n",
            "urllib3                            2.3.0\n",
            "vega-datasets                      0.9.0\n",
            "wadllib                            1.3.6\n",
            "wandb                              0.19.6\n",
            "wasabi                             1.1.3\n",
            "wcwidth                            0.2.13\n",
            "weasel                             0.4.1\n",
            "webcolors                          24.11.1\n",
            "webencodings                       0.5.1\n",
            "websocket-client                   1.8.0\n",
            "websockets                         14.2\n",
            "Werkzeug                           3.1.3\n",
            "wheel                              0.45.1\n",
            "widgetsnbextension                 3.6.10\n",
            "wordcloud                          1.9.4\n",
            "wrapt                              1.17.2\n",
            "xarray                             2025.1.2\n",
            "xarray-einstats                    0.8.0\n",
            "xgboost                            2.1.3\n",
            "xlrd                               2.0.1\n",
            "xxhash                             3.5.0\n",
            "xyzservices                        2025.1.0\n",
            "yarl                               1.18.3\n",
            "yellowbrick                        1.5\n",
            "yfinance                           0.2.52\n",
            "zipp                               3.21.0\n",
            "zstandard                          0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package                            Version\n",
        "---------------------------------- ------------------\n",
        "absl-py                            1.4.0\n",
        "accelerate                         1.3.0\n",
        "aiohappyeyeballs                   2.4.6\n",
        "aiohttp                            3.11.12\n",
        "aiosignal                          1.3.2\n",
        "alabaster                          1.0.0\n",
        "albucore                           0.0.23\n",
        "albumentations                     2.0.3\n",
        "ale-py                             0.10.1\n",
        "altair                             5.5.0\n",
        "annotated-types                    0.7.0\n",
        "anyio                              3.7.1\n",
        "argon2-cffi                        23.1.0\n",
        "argon2-cffi-bindings               21.2.0\n",
        "array_record                       0.6.0\n",
        "arviz                              0.20.0\n",
        "astropy                            7.0.0\n",
        "astropy-iers-data                  0.2025.2.3.0.32.42\n",
        "astunparse                         1.6.3\n",
        "atpublic                           4.1.0\n",
        "attrs                              25.1.0\n",
        "audioread                          3.0.1\n",
        "autograd                           1.7.0\n",
        "babel                              2.17.0\n",
        "backcall                           0.2.0\n",
        "beautifulsoup4                     4.13.3\n",
        "bigframes                          1.35.0\n",
        "bigquery-magics                    0.5.0\n",
        "bleach                             6.2.0\n",
        "blinker                            1.9.0\n",
        "blis                               0.7.11\n",
        "blosc2                             3.0.0\n",
        "bokeh                              3.6.3\n",
        "Bottleneck                         1.4.2\n",
        "bqplot                             0.12.44\n",
        "branca                             0.8.1\n",
        "CacheControl                       0.14.2\n",
        "cachetools                         5.5.1\n",
        "catalogue                          2.0.10\n",
        "certifi                            2025.1.31\n",
        "cffi                               1.17.1\n",
        "chardet                            5.2.0\n",
        "charset-normalizer                 3.4.1\n",
        "chex                               0.1.88\n",
        "clarabel                           0.10.0\n",
        "click                              8.1.8\n",
        "cloudpathlib                       0.20.0\n",
        "cloudpickle                        3.1.1\n",
        "cmake                              3.31.4\n",
        "cmdstanpy                          1.2.5\n",
        "colorcet                           3.1.0\n",
        "coloredlogs                        15.0.1\n",
        "colorlover                         0.3.0\n",
        "colour                             0.1.5\n",
        "community                          1.0.0b1\n",
        "confection                         0.1.5\n",
        "cons                               0.4.6\n",
        "contourpy                          1.3.1\n",
        "cramjam                            2.9.1\n",
        "cryptography                       43.0.3\n",
        "cuda-python                        12.6.0\n",
        "cudf-cu12                          24.12.0\n",
        "cufflinks                          0.17.3\n",
        "cupy-cuda12x                       13.3.0\n",
        "cvxopt                             1.3.2\n",
        "cvxpy                              1.6.0\n",
        "cycler                             0.12.1\n",
        "cyipopt                            1.5.0\n",
        "cymem                              2.0.11\n",
        "Cython                             3.0.11\n",
        "dask                               2024.10.0\n",
        "datascience                        0.17.6\n",
        "datasets                           3.2.0\n",
        "db-dtypes                          1.4.1\n",
        "dbus-python                        1.2.18\n",
        "debugpy                            1.8.0\n",
        "decorator                          4.4.2\n",
        "defusedxml                         0.7.1\n",
        "Deprecated                         1.2.18\n",
        "diffusers                          0.32.2\n",
        "dill                               0.3.8\n",
        "distro                             1.9.0\n",
        "dlib                               19.24.2\n",
        "dm-tree                            0.1.9\n",
        "docker-pycreds                     0.4.0\n",
        "docstring_parser                   0.16\n",
        "docutils                           0.21.2\n",
        "dopamine_rl                        4.1.2\n",
        "duckdb                             1.1.3\n",
        "earthengine-api                    1.5.1\n",
        "easydict                           1.13\n",
        "editdistance                       0.8.1\n",
        "eerepr                             0.1.0\n",
        "einops                             0.8.0\n",
        "en-core-web-sm                     3.7.1\n",
        "entrypoints                        0.4\n",
        "et_xmlfile                         2.0.0\n",
        "etils                              1.11.0\n",
        "etuples                            0.3.9\n",
        "evaluate                           0.4.3\n",
        "Farama-Notifications               0.0.4\n",
        "fastai                             2.7.18\n",
        "fastcore                           1.7.29\n",
        "fastdownload                       0.0.7\n",
        "fastjsonschema                     2.21.1\n",
        "fastprogress                       1.0.3\n",
        "fastrlock                          0.8.3\n",
        "filelock                           3.17.0\n",
        "firebase-admin                     6.6.0\n",
        "Flask                              3.1.0\n",
        "flatbuffers                        25.2.10\n",
        "flax                               0.10.2\n",
        "folium                             0.19.4\n",
        "fonttools                          4.55.8\n",
        "frozendict                         2.4.6\n",
        "frozenlist                         1.5.0\n",
        "fsspec                             2024.9.0\n",
        "future                             1.0.0\n",
        "gast                               0.6.0\n",
        "gcsfs                              2024.10.0\n",
        "GDAL                               3.6.4\n",
        "gdown                              5.2.0\n",
        "geemap                             0.35.1\n",
        "gensim                             4.3.3\n",
        "geocoder                           1.38.1\n",
        "geographiclib                      2.0\n",
        "geopandas                          1.0.1\n",
        "geopy                              2.4.1\n",
        "gin-config                         0.5.0\n",
        "gitdb                              4.0.12\n",
        "GitPython                          3.1.44\n",
        "glob2                              0.7\n",
        "google                             2.0.3\n",
        "google-ai-generativelanguage       0.6.15\n",
        "google-api-core                    2.19.2\n",
        "google-api-python-client           2.160.0\n",
        "google-auth                        2.27.0\n",
        "google-auth-httplib2               0.2.0\n",
        "google-auth-oauthlib               1.2.1\n",
        "google-cloud-aiplatform            1.79.0\n",
        "google-cloud-bigquery              3.25.0\n",
        "google-cloud-bigquery-connection   1.17.0\n",
        "google-cloud-bigquery-storage      2.28.0\n",
        "google-cloud-bigtable              2.28.1\n",
        "google-cloud-core                  2.4.1\n",
        "google-cloud-dataproc              5.16.0\n",
        "google-cloud-datastore             2.20.2\n",
        "google-cloud-firestore             2.20.0\n",
        "google-cloud-functions             1.19.0\n",
        "google-cloud-iam                   2.17.0\n",
        "google-cloud-language              2.16.0\n",
        "google-cloud-pubsub                2.25.0\n",
        "google-cloud-resource-manager      1.14.0\n",
        "google-cloud-spanner               3.51.0\n",
        "google-cloud-storage               2.19.0\n",
        "google-cloud-translate             3.19.0\n",
        "google-colab                       1.0.0\n",
        "google-crc32c                      1.6.0\n",
        "google-genai                       0.8.0\n",
        "google-generativeai                0.8.4\n",
        "google-pasta                       0.2.0\n",
        "google-resumable-media             2.7.2\n",
        "google-spark-connect               0.5.1\n",
        "googleapis-common-protos           1.66.0\n",
        "googledrivedownloader              1.1.0\n",
        "graphviz                           0.20.3\n",
        "greenlet                           3.1.1\n",
        "grpc-google-iam-v1                 0.14.0\n",
        "grpc-interceptor                   0.15.4\n",
        "grpcio                             1.70.0\n",
        "grpcio-status                      1.62.3\n",
        "gspread                            6.1.4\n",
        "gspread-dataframe                  4.0.0\n",
        "gym                                0.25.2\n",
        "gym-notices                        0.0.8\n",
        "gymnasium                          1.0.0\n",
        "h11                                0.14.0\n",
        "h5netcdf                           1.5.0\n",
        "h5py                               3.12.1\n",
        "highspy                            1.9.0\n",
        "holidays                           0.66\n",
        "holoviews                          1.20.0\n",
        "html5lib                           1.1\n",
        "httpcore                           1.0.7\n",
        "httpimport                         1.4.0\n",
        "httplib2                           0.22.0\n",
        "httpx                              0.28.1\n",
        "huggingface-hub                    0.28.1\n",
        "humanfriendly                      10.0\n",
        "humanize                           4.11.0\n",
        "hyperopt                           0.2.7\n",
        "ibis-framework                     9.2.0\n",
        "idna                               3.10\n",
        "imageio                            2.37.0\n",
        "imageio-ffmpeg                     0.6.0\n",
        "imagesize                          1.4.1\n",
        "imbalanced-learn                   0.13.0\n",
        "imgaug                             0.4.0\n",
        "immutabledict                      4.2.1\n",
        "importlib_metadata                 8.6.1\n",
        "importlib_resources                6.5.2\n",
        "imutils                            0.5.4\n",
        "inflect                            7.5.0\n",
        "iniconfig                          2.0.0\n",
        "intel-cmplr-lib-ur                 2025.0.4\n",
        "intel-openmp                       2025.0.4\n",
        "ipyevents                          2.0.2\n",
        "ipyfilechooser                     0.6.0\n",
        "ipykernel                          5.5.6\n",
        "ipyleaflet                         0.19.2\n",
        "ipyparallel                        8.8.0\n",
        "ipython                            7.34.0\n",
        "ipython-genutils                   0.2.0\n",
        "ipython-sql                        0.5.0\n",
        "ipytree                            0.2.2\n",
        "ipywidgets                         7.7.1\n",
        "itsdangerous                       2.2.0\n",
        "jax                                0.4.33\n",
        "jax-cuda12-pjrt                    0.4.33\n",
        "jax-cuda12-plugin                  0.4.33\n",
        "jaxlib                             0.4.33\n",
        "jeepney                            0.7.1\n",
        "jellyfish                          1.1.0\n",
        "jieba                              0.42.1\n",
        "Jinja2                             3.1.5\n",
        "jiter                              0.8.2\n",
        "joblib                             1.4.2\n",
        "jsonpatch                          1.33\n",
        "jsonpickle                         4.0.1\n",
        "jsonpointer                        3.0.0\n",
        "jsonschema                         4.23.0\n",
        "jsonschema-specifications          2024.10.1\n",
        "jupyter-client                     6.1.12\n",
        "jupyter-console                    6.1.0\n",
        "jupyter_core                       5.7.2\n",
        "jupyter-leaflet                    0.19.2\n",
        "jupyter-server                     1.24.0\n",
        "jupyterlab_pygments                0.3.0\n",
        "jupyterlab_widgets                 3.0.13\n",
        "kaggle                             1.6.17\n",
        "kagglehub                          0.3.6\n",
        "keras                              3.8.0\n",
        "keras-hub                          0.18.1\n",
        "keras-nlp                          0.18.1\n",
        "keyring                            23.5.0\n",
        "kiwisolver                         1.4.8\n",
        "langchain                          0.3.17\n",
        "langchain-core                     0.3.33\n",
        "langchain-text-splitters           0.3.5\n",
        "langcodes                          3.5.0\n",
        "langsmith                          0.3.6\n",
        "language_data                      1.3.0\n",
        "launchpadlib                       1.10.16\n",
        "lazr.restfulclient                 0.14.4\n",
        "lazr.uri                           1.0.6\n",
        "lazy_loader                        0.4\n",
        "libclang                           18.1.1\n",
        "libcudf-cu12                       24.12.0\n",
        "libkvikio-cu12                     24.12.1\n",
        "librosa                            0.10.2.post1\n",
        "lightgbm                           4.5.0\n",
        "linkify-it-py                      2.0.3\n",
        "llvmlite                           0.44.0\n",
        "locket                             1.0.0\n",
        "logical-unification                0.4.6\n",
        "lxml                               5.3.0\n",
        "marisa-trie                        1.2.1\n",
        "Markdown                           3.7\n",
        "markdown-it-py                     3.0.0\n",
        "MarkupSafe                         3.0.2\n",
        "matplotlib                         3.10.0\n",
        "matplotlib-inline                  0.1.7\n",
        "matplotlib-venn                    1.1.1\n",
        "mdit-py-plugins                    0.4.2\n",
        "mdurl                              0.1.2\n",
        "miniKanren                         1.0.3\n",
        "missingno                          0.5.2\n",
        "mistune                            3.1.1\n",
        "mizani                             0.13.1\n",
        "mkl                                2025.0.1\n",
        "ml-dtypes                          0.4.1\n",
        "mlxtend                            0.23.4\n",
        "more-itertools                     10.6.0\n",
        "moviepy                            1.0.3\n",
        "mpmath                             1.3.0\n",
        "msgpack                            1.1.0\n",
        "multidict                          6.1.0\n",
        "multipledispatch                   1.0.0\n",
        "multiprocess                       0.70.16\n",
        "multitasking                       0.0.11\n",
        "murmurhash                         1.0.12\n",
        "music21                            9.3.0\n",
        "namex                              0.0.8\n",
        "narwhals                           1.25.1\n",
        "natsort                            8.4.0\n",
        "nbclassic                          1.2.0\n",
        "nbclient                           0.10.2\n",
        "nbconvert                          7.16.6\n",
        "nbformat                           5.10.4\n",
        "ndindex                            1.9.2\n",
        "nest-asyncio                       1.6.0\n",
        "networkx                           3.4.2\n",
        "nibabel                            5.3.2\n",
        "nltk                               3.9.1\n",
        "notebook                           6.5.5\n",
        "notebook_shim                      0.2.4\n",
        "numba                              0.61.0\n",
        "numba-cuda                         0.0.17.1\n",
        "numexpr                            2.10.2\n",
        "numpy                              2.2.2\n",
        "nvidia-cublas-cu12                 12.4.5.8\n",
        "nvidia-cuda-cupti-cu12             12.4.127\n",
        "nvidia-cuda-nvcc-cu12              12.5.82\n",
        "nvidia-cuda-nvrtc-cu12             12.4.127\n",
        "nvidia-cuda-runtime-cu12           12.4.127\n",
        "nvidia-cudnn-cu12                  9.1.0.70\n",
        "nvidia-cufft-cu12                  11.2.1.3\n",
        "nvidia-curand-cu12                 10.3.5.147\n",
        "nvidia-cusolver-cu12               11.6.1.9\n",
        "nvidia-cusparse-cu12               12.3.1.170\n",
        "nvidia-cusparselt-cu12             0.6.2\n",
        "nvidia-nccl-cu12                   2.21.5\n",
        "nvidia-nvcomp-cu12                 4.1.0.6\n",
        "nvidia-nvjitlink-cu12              12.4.127\n",
        "nvidia-nvtx-cu12                   12.4.127\n",
        "nvtx                               0.2.10\n",
        "nx-cugraph-cu12                    24.12.0\n",
        "oauth2client                       4.1.3\n",
        "oauthlib                           3.2.2\n",
        "onnx                               1.17.0\n",
        "onnxruntime                        1.20.1\n",
        "onnxruntime-genai                  0.5.2\n",
        "openai                             1.61.1\n",
        "opencv-contrib-python              4.11.0.86\n",
        "opencv-python                      4.11.0.86\n",
        "opencv-python-headless             4.11.0.86\n",
        "openpyxl                           3.1.5\n",
        "opentelemetry-api                  1.16.0\n",
        "opentelemetry-sdk                  1.16.0\n",
        "opentelemetry-semantic-conventions 0.37b0\n",
        "opt_einsum                         3.4.0\n",
        "optax                              0.2.4\n",
        "optimum                            1.24.0\n",
        "optree                             0.14.0\n",
        "orbax-checkpoint                   0.6.4\n",
        "orjson                             3.10.15\n",
        "osqp                               0.6.7.post3\n",
        "packaging                          24.2\n",
        "pandas                             2.2.3\n",
        "pandas-datareader                  0.10.0\n",
        "pandas-gbq                         0.26.1\n",
        "pandas-stubs                       2.2.2.240909\n",
        "pandocfilters                      1.5.1\n",
        "panel                              1.6.0\n",
        "param                              2.2.0\n",
        "parso                              0.8.4\n",
        "parsy                              2.1\n",
        "partd                              1.4.2\n",
        "pathlib                            1.0.1\n",
        "patsy                              1.0.1\n",
        "peewee                             3.17.9\n",
        "peft                               0.14.0\n",
        "pexpect                            4.9.0\n",
        "pickleshare                        0.7.5\n",
        "pillow                             11.1.0\n",
        "pip                                24.1.2\n",
        "platformdirs                       4.3.6\n",
        "plotly                             5.24.1\n",
        "plotnine                           0.14.5\n",
        "pluggy                             1.5.0\n",
        "ply                                3.11\n",
        "polars                             1.9.0\n",
        "pooch                              1.8.2\n",
        "portpicker                         1.5.2\n",
        "preshed                            3.0.9\n",
        "prettytable                        3.14.0\n",
        "proglog                            0.1.10\n",
        "progressbar2                       4.5.0\n",
        "prometheus_client                  0.21.1\n",
        "promise                            2.3\n",
        "prompt_toolkit                     3.0.50\n",
        "propcache                          0.2.1\n",
        "prophet                            1.1.6\n",
        "proto-plus                         1.26.0\n",
        "protobuf                           5.29.3\n",
        "psutil                             5.9.5\n",
        "psycopg2                           2.9.10\n",
        "ptyprocess                         0.7.0\n",
        "py-cpuinfo                         9.0.0\n",
        "py4j                               0.10.9.7\n",
        "pyarrow                            19.0.0\n",
        "pyasn1                             0.6.1\n",
        "pyasn1_modules                     0.4.1\n",
        "pycocotools                        2.0.8\n",
        "pycparser                          2.22\n",
        "pydantic                           2.10.6\n",
        "pydantic_core                      2.27.2\n",
        "pydata-google-auth                 1.9.1\n",
        "pydot                              3.0.4\n",
        "pydotplus                          2.0.2\n",
        "PyDrive                            1.3.1\n",
        "PyDrive2                           1.21.3\n",
        "pyerfa                             2.0.1.5\n",
        "pygame                             2.6.1\n",
        "pygit2                             1.17.0\n",
        "Pygments                           2.18.0\n",
        "PyGObject                          3.42.1\n",
        "PyJWT                              2.10.1\n",
        "pylibcudf-cu12                     24.12.0\n",
        "pylibcugraph-cu12                  24.12.0\n",
        "pylibraft-cu12                     24.12.0\n",
        "pymc                               5.20.0\n",
        "pymystem3                          0.2.0\n",
        "pynvjitlink-cu12                   0.5.0\n",
        "pyogrio                            0.10.0\n",
        "Pyomo                              6.8.2\n",
        "PyOpenGL                           3.1.9\n",
        "pyOpenSSL                          24.2.1\n",
        "pyparsing                          3.2.1\n",
        "pyperclip                          1.9.0\n",
        "pyproj                             3.7.0\n",
        "pyshp                              2.3.1\n",
        "PySocks                            1.7.1\n",
        "pyspark                            3.5.4\n",
        "pytensor                           2.26.4\n",
        "pytest                             8.3.4\n",
        "python-apt                         0.0.0\n",
        "python-box                         7.3.2\n",
        "python-dateutil                    2.9.0.post0\n",
        "python-louvain                     0.16\n",
        "python-slugify                     8.0.4\n",
        "python-snappy                      0.7.3\n",
        "python-utils                       3.9.1\n",
        "pytz                               2025.1\n",
        "pyviz_comms                        3.0.4\n",
        "PyYAML                             6.0.2\n",
        "pyzmq                              24.0.1\n",
        "qdldl                              0.1.7.post5\n",
        "ratelim                            0.1.6\n",
        "referencing                        0.36.2\n",
        "regex                              2024.11.6\n",
        "requests                           2.32.3\n",
        "requests-oauthlib                  2.0.0\n",
        "requests-toolbelt                  1.0.0\n",
        "requirements-parser                0.9.0\n",
        "rich                               13.9.4\n",
        "rmm-cu12                           24.12.1\n",
        "rpds-py                            0.22.3\n",
        "rpy2                               3.4.2\n",
        "rsa                                4.9\n",
        "safetensors                        0.5.2\n",
        "scikit-image                       0.25.1\n",
        "scikit-learn                       1.6.1\n",
        "scipy                              1.13.1\n",
        "scooby                             0.10.0\n",
        "scs                                3.2.7.post2\n",
        "seaborn                            0.13.2\n",
        "SecretStorage                      3.3.1\n",
        "Send2Trash                         1.8.3\n",
        "sentence-transformers              3.4.1\n",
        "sentencepiece                      0.2.0\n",
        "sentry-sdk                         2.20.0\n",
        "setproctitle                       1.3.4\n",
        "setuptools                         75.1.0\n",
        "shap                               0.46.0\n",
        "shapely                            2.0.7\n",
        "shellingham                        1.5.4\n",
        "simple-parsing                     0.1.7\n",
        "simsimd                            6.2.1\n",
        "six                                1.17.0\n",
        "sklearn-compat                     0.1.3\n",
        "sklearn-pandas                     2.2.0\n",
        "slicer                             0.0.8\n",
        "smart-open                         7.1.0\n",
        "smmap                              5.0.2\n",
        "sniffio                            1.3.1\n",
        "snowballstemmer                    2.2.0\n",
        "soundfile                          0.13.1\n",
        "soupsieve                          2.6\n",
        "soxr                               0.5.0.post1\n",
        "spacy                              3.7.5\n",
        "spacy-legacy                       3.0.12\n",
        "spacy-loggers                      1.0.5\n",
        "spanner-graph-notebook             1.0.9\n",
        "Sphinx                             8.1.3\n",
        "sphinxcontrib-applehelp            2.0.0\n",
        "sphinxcontrib-devhelp              2.0.0\n",
        "sphinxcontrib-htmlhelp             2.1.0\n",
        "sphinxcontrib-jsmath               1.0.1\n",
        "sphinxcontrib-qthelp               2.0.0\n",
        "sphinxcontrib-serializinghtml      2.0.0\n",
        "SQLAlchemy                         2.0.37\n",
        "sqlglot                            25.6.1\n",
        "sqlparse                           0.5.3\n",
        "srsly                              2.5.1\n",
        "stanio                             0.5.1\n",
        "statsmodels                        0.14.4\n",
        "stringzilla                        3.11.3\n",
        "sympy                              1.13.1\n",
        "tables                             3.10.2\n",
        "tabulate                           0.9.0\n",
        "tbb                                2022.0.0\n",
        "tcmlib                             1.2.0\n",
        "tenacity                           9.0.0\n",
        "tensorboard                        2.18.0\n",
        "tensorboard-data-server            0.7.2\n",
        "tensorflow                         2.18.0\n",
        "tensorflow-datasets                4.9.7\n",
        "tensorflow-hub                     0.16.1\n",
        "tensorflow-io-gcs-filesystem       0.37.1\n",
        "tensorflow-metadata                1.16.1\n",
        "tensorflow-probability             0.25.0\n",
        "tensorflow-text                    2.18.1\n",
        "tensorstore                        0.1.71\n",
        "termcolor                          2.5.0\n",
        "terminado                          0.18.1\n",
        "text-unidecode                     1.3\n",
        "textblob                           0.19.0\n",
        "tf_keras                           2.18.0\n",
        "tf-slim                            1.1.0\n",
        "thinc                              8.2.5\n",
        "threadpoolctl                      3.5.0\n",
        "tifffile                           2025.1.10\n",
        "timm                               1.0.14\n",
        "tinycss2                           1.4.0\n",
        "tokenizers                         0.21.0\n",
        "toml                               0.10.2\n",
        "toolz                              0.12.1\n",
        "torch                              2.6.0\n",
        "torchaudio                         2.5.1+cu124\n",
        "torchsummary                       1.5.1\n",
        "torchvision                        0.21.0\n",
        "tornado                            6.4.2\n",
        "tqdm                               4.67.1\n",
        "traitlets                          5.7.1\n",
        "traittypes                         0.2.1\n",
        "transformers                       4.48.3\n",
        "triton                             3.2.0\n",
        "tweepy                             4.15.0\n",
        "typeguard                          4.4.1\n",
        "typer                              0.15.1\n",
        "types-pytz                         2025.1.0.20250204\n",
        "types-setuptools                   75.8.0.20250210\n",
        "typing_extensions                  4.12.2\n",
        "tzdata                             2025.1\n",
        "tzlocal                            5.2\n",
        "uc-micro-py                        1.0.3\n",
        "umf                                0.9.1\n",
        "uritemplate                        4.1.1\n",
        "urllib3                            2.3.0\n",
        "vega-datasets                      0.9.0\n",
        "wadllib                            1.3.6\n",
        "wandb                              0.19.6\n",
        "wasabi                             1.1.3\n",
        "wcwidth                            0.2.13\n",
        "weasel                             0.4.1\n",
        "webcolors                          24.11.1\n",
        "webencodings                       0.5.1\n",
        "websocket-client                   1.8.0\n",
        "websockets                         14.2\n",
        "Werkzeug                           3.1.3\n",
        "wheel                              0.45.1\n",
        "widgetsnbextension                 3.6.10\n",
        "wordcloud                          1.9.4\n",
        "wrapt                              1.17.2\n",
        "xarray                             2025.1.2\n",
        "xarray-einstats                    0.8.0\n",
        "xgboost                            2.1.3\n",
        "xlrd                               2.0.1\n",
        "xxhash                             3.5.0\n",
        "xyzservices                        2025.1.0\n",
        "yarl                               1.18.3\n",
        "yellowbrick                        1.5\n",
        "yfinance                           0.2.52\n",
        "zipp                               3.21.0\n",
        "zstandard                          0.23.0"
      ],
      "metadata": {
        "id": "s3-y9QSvLfOj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HUiixP83Lerx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX9Ce7NhLMxK",
        "outputId": "f6e5c37f-1711-46cb-fd44-b120ec896144"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[0mcontent@ /content\u001b[0m\n",
            "\u001b[0m├── @huggingface/transformers@3.3.3\u001b[0m\n",
            "\u001b[0m├── @xenova/transformers@2.17.2\u001b[0m\n",
            "\u001b[0m├── onnxruntime-node@1.20.1\u001b[0m\n",
            "\u001b[0m└── onnxruntime-web@1.21.0-dev.20250206-d981b153d3\u001b[0m\n",
            "\u001b[0m\u001b[0m\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⠙content@ /content\n",
        "├── @huggingface/transformers@3.3.3\n",
        "├── @xenova/transformers@2.17.2\n",
        "├── onnxruntime-node@1.20.1\n",
        "└── onnxruntime-web@1.21.0-dev.20250206-d981b153d3"
      ],
      "metadata": {
        "id": "g9He5tbCLYWc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HebsdAIRLQkB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}